import os
import asyncio
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor
import threading

from fastapi import FastAPI, HTTPException, Query, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from video_keyword_store import VideoKeywordStore
from graph_context_service import GraphContextService
from video_registry import VideoRegistry

# ============================================
# LLM ìš”ì²­ í ì‹œìŠ¤í…œ
# ============================================
class LLMRequestQueue:
    """LLM ì¶”ë¡  ìš”ì²­ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_concurrent: int = 1):
        self.queue = asyncio.Queue()
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self._executor = ThreadPoolExecutor(max_workers=max_concurrent)
        self._started = False
    
    async def submit(self, func, *args, **kwargs):
        """LLM ìš”ì²­ì„ íì— ì œì¶œí•˜ê³  ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼"""
        async with self.semaphore:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self._executor, 
                lambda: func(*args, **kwargs)
            )
            return result
    
    def shutdown(self):
        self._executor.shutdown(wait=False)

# ì „ì—­ LLM í
_llm_queue: Optional[LLMRequestQueue] = None

def get_llm_queue() -> LLMRequestQueue:
    global _llm_queue
    if _llm_queue is None:
        _llm_queue = LLMRequestQueue(max_concurrent=2)
    return _llm_queue


# ============================================
# í”„ë¦¬ë¡œë“œ ìºì‹œ
# ============================================
_preload_cache: dict = {}
_preload_status: dict = {}


# ============================================
# Pydantic ëª¨ë¸
# ============================================
class QueryRequest(BaseModel):
    query: str
    video_id: Optional[str] = None
    focus_keywords: Optional[List[str]] = None


class QueryResponse(BaseModel):
    query: str
    answer: str


class VideoRegisterRequest(BaseModel):
    video_id: str
    keyword_path: str


class VideoStreamRequest(BaseModel):
    video_id: str
    interval: float = 1.0


# ============================================
# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# ============================================
_graph_service = None
_keyword_store = None
_registry = None
_init_lock = threading.Lock()

def get_services():
    """ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ê³  ì¬ì‚¬ìš© (thread-safe)"""
    global _graph_service, _keyword_store, _registry
    
    if _graph_service is None:
        with _init_lock:
            if _graph_service is None:
                print("\n[ì´ˆê¸°í™”] GraphRAG ì„œë¹„ìŠ¤ ìµœì´ˆ ë¡œë“œ ì¤‘...")
                graphrag_config = {
                    "embedding_model_name": os.environ.get("EMBED_MODEL", "intfloat/multilingual-e5-large-instruct"),
                    "llm_model_name": os.environ.get("LLM_MODEL", "gemma3:12b"),
                    "arango_host": os.environ.get("ARANGO_HOST", "localhost"),
                    "arango_port": int(os.environ.get("ARANGO_PORT", "8530")),
                    "arango_password": os.environ.get("ARANGO_PASSWORD", ""),
                    "arango_db_name": os.environ.get("ARANGO_DB", "knowledge_graph"),
                    "arango_reset": False,
                    "global_arango_db_name": os.environ.get("GLOBAL_GRAPH_DB", "knowledge_graph"),
                    "global_arango_reset": False,
                    "use_reranker": True,
                    "use_tika": False
                }
                index_dir = os.environ.get("GRAPHRAG_INDEX_DIR", "graphrag_data/global")
                keyword_root = os.environ.get("VIDEO_KEYWORD_DIR", "video_keywords")
                
                _keyword_store = VideoKeywordStore(keyword_root)
                _registry = VideoRegistry(os.environ.get("VIDEO_REGISTRY_FILE", os.path.join(keyword_root, "registry.json")))
                _graph_service = GraphContextService(graphrag_config, index_dir=index_dir)
                print("[ì™„ë£Œ] GraphRAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    return _graph_service, _keyword_store, _registry


# ============================================
# ë¹„ë™ê¸° í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================
_thread_pool = ThreadPoolExecutor(max_workers=8)

async def run_in_thread(func, *args, **kwargs):
    """ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(_thread_pool, lambda: func(*args, **kwargs))


# ============================================
# FastAPI ì•± ìƒì„±
# ============================================
def create_app() -> FastAPI:
    app = FastAPI(title="Real-time GraphRAG API (Async)")

    # CORS ì„¤ì •
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        allow_headers=["*"],
        expose_headers=["*"],
    )

    # ========================================
    # ë¹„ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ë“¤
    # ========================================
    
    @app.get("/videos/registry")
    async def list_registry():
        """Registryì— ë“±ë¡ëœ ëª¨ë“  ì˜ìƒ ëª©ë¡ ì¡°íšŒ"""
        graph_service, keyword_store, registry = get_services()
        videos = await run_in_thread(registry.list_videos)
        return {"videos": videos}

    @app.post("/videos/register")
    async def register_video(payload: VideoRegisterRequest):
        graph_service, keyword_store, registry = get_services()
        try:
            await run_in_thread(registry.register, payload.video_id, payload.keyword_path)
        except FileNotFoundError as exc:
            raise HTTPException(status_code=400, detail=str(exc))
        
        resolved = await run_in_thread(registry.resolve, payload.video_id)
        return {
            "video_id": payload.video_id,
            "keyword_path": resolved
        }

    @app.get("/videos/{video_id:path}/keywords")
    async def get_keywords(
        video_id: str,
        start: Optional[float] = Query(None),
        end: Optional[float] = Query(None),
        top_k: int = Query(10, ge=1, le=20)
    ):
        graph_service, keyword_store, registry = get_services()
        keyword_path = await run_in_thread(registry.resolve, video_id)
        
        if not keyword_path:
            raise HTTPException(
                status_code=404, 
                detail=f"ì˜ìƒ '{video_id}'ì— ëŒ€í•œ í‚¤ì›Œë“œ JSONì´ registryì— ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        # ìºì‹œ í™•ì¸
        slice_duration = 15
        if start is not None:
            slice_index = int(start // slice_duration)
            cache_key = (video_id, slice_index)
            
            if cache_key in _preload_cache:
                cached = _preload_cache[cache_key]
                print(f"[Cache HIT] video={video_id}, slice={slice_index}")
                return {
                    "video_id": video_id,
                    "start": start,
                    "end": end,
                    "keywords": cached["keywords"],
                    "entities": cached.get("entities", []),
                    "mapped_entities": cached["mapped_entities"],
                    "keyword_path": keyword_path,
                    "slice_count": 1,
                    "from_cache": True
                }
        
        try:
            # ë¹„ë™ê¸°ë¡œ í‚¤ì›Œë“œ ì¡°íšŒ
            window = await run_in_thread(
                keyword_store.query,
                video_id,
                start,
                end,
                top_k,
                keyword_path
            )
        except FileNotFoundError as e:
            raise HTTPException(
                status_code=404, 
                detail=f"í‚¤ì›Œë“œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {keyword_path}"
            )

        keywords = [item["term"] for item in window["keywords"]]
        
        # ì—”í‹°í‹° ë§¤í•‘ (ë¹„ë™ê¸°)
        mapped_entities = await run_in_thread(
            graph_service.map_keywords_to_entities, 
            keywords, 
            top_k
        )
        
        window["mapped_entities"] = mapped_entities
        window["keyword_path"] = keyword_path
        window["from_cache"] = False
        return window

    @app.post("/videos/{video_id:path}/preload")
    async def preload_video_keywords(
        video_id: str,
        top_k: int = Query(10, ge=1, le=20)
    ):
        """ì˜ìƒì˜ ëª¨ë“  í‚¤ì›Œë“œ ìŠ¬ë¼ì´ìŠ¤ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¯¸ë¦¬ ë¡œë“œ"""
        graph_service, keyword_store, registry = get_services()
        keyword_path = await run_in_thread(registry.resolve, video_id)
        
        if not keyword_path:
            raise HTTPException(
                status_code=404,
                detail=f"ì˜ìƒ '{video_id}'ì— ëŒ€í•œ í‚¤ì›Œë“œ JSONì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        # ì´ë¯¸ ë¡œë”© ì¤‘ì´ë©´ ìƒíƒœë§Œ ë°˜í™˜
        if video_id in _preload_status and _preload_status[video_id].get("is_loading"):
            return {
                "video_id": video_id,
                "status": "loading",
                **_preload_status[video_id]
            }
        
        try:
            slices = await run_in_thread(
                keyword_store.load_slices, 
                video_id, 
                keyword_path
            )
        except FileNotFoundError as e:
            raise HTTPException(status_code=404, detail=f"í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        
        total_slices = len(slices)
        cached_count = sum(1 for i in range(total_slices) if (video_id, i) in _preload_cache)
        
        if cached_count == total_slices:
            return {
                "video_id": video_id,
                "status": "complete",
                "total_slices": total_slices,
                "loaded_slices": total_slices,
                "is_loading": False
            }
        
        _preload_status[video_id] = {
            "total_slices": total_slices,
            "loaded_slices": cached_count,
            "is_loading": True
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ í”„ë¦¬ë¡œë“œ
        asyncio.create_task(_preload_slices_async(
            video_id, slices, graph_service, top_k
        ))
        
        return {
            "video_id": video_id,
            "status": "started",
            "total_slices": total_slices,
            "loaded_slices": cached_count,
            "is_loading": True
        }

    async def _preload_slices_async(video_id: str, slices, graph_service, top_k: int):
        """ë¹„ë™ê¸° í”„ë¦¬ë¡œë“œ ì²˜ë¦¬"""
        print(f"[Preload] ì‹œì‘: video={video_id}, ì´ {len(slices)}ê°œ ìŠ¬ë¼ì´ìŠ¤")
        
        for idx, sl in enumerate(slices):
            cache_key = (video_id, idx)
            
            if cache_key in _preload_cache:
                continue
            
            try:
                # children í¬í•¨í•˜ì—¬ í‚¤ì›Œë“œ ì €ì¥
                keywords = []
                for k in sl.keywords:
                    term = k.get("term") or k.get("keyword")
                    if not term:
                        continue
                    kw_item = {
                        "term": term,
                        "score": k.get("score", 1.0)
                    }
                    # children, start, end í¬í•¨
                    if "children" in k:
                        kw_item["children"] = k["children"]
                    if "start" in k:
                        kw_item["start"] = k["start"]
                    if "end" in k:
                        kw_item["end"] = k["end"]
                    keywords.append(kw_item)
                
                if keywords:
                    keyword_terms = [k["term"] for k in keywords]
                    mapped_entities = await run_in_thread(
                        graph_service.map_keywords_to_entities, 
                        keyword_terms, 
                        top_k
                    )
                else:
                    mapped_entities = []
                
                _preload_cache[cache_key] = {
                    "keywords": keywords,
                    "entities": [{"name": e.get("name"), "score": e.get("score", 1.0)} for e in sl.entities] if sl.entities else [],
                    "mapped_entities": mapped_entities,
                    "slice_start": sl.start,
                    "slice_end": sl.end
                }
                
                # children í¬í•¨ ì—¬ë¶€ ë¡œê·¸
                for kw in keywords:
                    if "children" in kw:
                        print(f"[Preload] '{kw['term']}'ì˜ children: {[c.get('term') for c in kw['children']]}")
                
                _preload_status[video_id]["loaded_slices"] = idx + 1
                print(f"[Preload] ì™„ë£Œ: slice {idx+1}/{len(slices)}")
                
            except Exception as e:
                print(f"[Preload] ì˜¤ë¥˜ slice {idx}: {e}")
        
        _preload_status[video_id]["is_loading"] = False
        print(f"[Preload] ì™„ë£Œ: video={video_id}")

    @app.get("/videos/{video_id:path}/preload-status")
    async def get_preload_status(video_id: str):
        """í”„ë¦¬ë¡œë“œ ì§„í–‰ ìƒíƒœ í™•ì¸"""
        if video_id not in _preload_status:
            return {
                "video_id": video_id,
                "status": "not_started",
                "total_slices": 0,
                "loaded_slices": 0,
                "is_loading": False
            }
        
        status = _preload_status[video_id]
        return {
            "video_id": video_id,
            "status": "complete" if not status["is_loading"] and status["loaded_slices"] == status["total_slices"] else "loading",
            **status
        }

    @app.get("/entity/{entity_name}")
    async def get_entity(entity_name: str, depth: int = Query(1, ge=1, le=3)):
        graph_service, keyword_store, registry = get_services()
        
        # ë¹„ë™ê¸°ë¡œ ì—”í‹°í‹° ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
        context = await run_in_thread(
            graph_service.get_entity_context, 
            entity_name, 
            depth
        )
        
        entity = context.get("entity")
        neighbors = context.get("neighbors", {})
        documents = []
        faiss_fallback = False
        
        # ì—”í‹°í‹°ê°€ ìˆê³  sourcesê°€ ìˆëŠ” ê²½ìš°
        if entity:
            sources = entity.get("sources", [])
            if sources and len(sources) > 0:
                # GraphDB sources ì‚¬ìš©
                documents = await run_in_thread(
                    graph_service.get_documents_for_entity, 
                    entity_name, 
                    5
                )
            else:
                # sourcesê°€ ì—†ìœ¼ë©´ FAISS ê²€ìƒ‰
                print(f"[entity] '{entity_name}' sources ì—†ìŒ - FAISS ê²€ìƒ‰")
                faiss_fallback = True
        else:
            # ì—”í‹°í‹°ê°€ ì—†ìœ¼ë©´ FAISS ê²€ìƒ‰
            print(f"[entity] '{entity_name}' GraphDBì— ì—†ìŒ - FAISS ê²€ìƒ‰")
            faiss_fallback = True
        
        # FAISS í´ë°± ê²€ìƒ‰
        if faiss_fallback:
            try:
                faiss_results = await run_in_thread(
                    graph_service.search_documents_faiss,
                    entity_name,
                    5
                )
                
                # FAISS ê²°ê³¼ë¥¼ documents í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                for doc in faiss_results:
                    content = doc.get("content", "")
                    source = doc.get("source", "")
                    doc_name = source.split("/")[-1].split("\\")[-1] if source else "ì•Œ ìˆ˜ ì—†ìŒ"
                    
                    documents.append({
                        "content": content,
                        "metadata": {
                            "source": source,
                            "doc": doc_name,
                            "from_faiss": True
                        }
                    })
                
                print(f"[entity] FAISSì—ì„œ {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
            except Exception as e:
                print(f"[entity] FAISS ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ì—”í‹°í‹°ê°€ ì—†ì–´ë„ FAISS ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°˜í™˜
        if not entity and not documents:
            raise HTTPException(status_code=404, detail="ì—”í‹°í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return {
            "entity": entity,
            "neighbors": neighbors,
            "documents": documents,
            "faiss_fallback": faiss_fallback
        }

    @app.post("/query", response_model=QueryResponse)
    async def run_query(payload: QueryRequest):
        """LLM ì§ˆì˜ - í ì‹œìŠ¤í…œì„ í†µí•´ ìˆœì°¨ ì²˜ë¦¬"""
        graph_service, keyword_store, registry = get_services()
        llm_queue = get_llm_queue()
        
        # LLM íë¥¼ í†µí•´ ìˆœì°¨ ì²˜ë¦¬ (ë™ì‹œ ìš”ì²­ ì œí•œ)
        try:
            answer = await llm_queue.submit(
                graph_service.answer_query, 
                payload.query
            )
            return QueryResponse(**answer)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

    @app.post("/videos/upload")
    async def upload_video(
        video_id: str,
        file: UploadFile = File(...)
    ):
        """ì˜ìƒ íŒŒì¼ ì—…ë¡œë“œ ë° í‚¤ì›Œë“œ JSON ìë™ ì—°ê²°"""
        import hashlib
        
        graph_service, keyword_store, registry = get_services()
        
        upload_dir = os.environ.get("VIDEO_UPLOAD_DIR", "uploaded_videos")
        os.makedirs(upload_dir, exist_ok=True)

        if video_id.startswith('http'):
            safe_filename = hashlib.md5(video_id.encode()).hexdigest()
        else:
            safe_filename = video_id
        
        video_filename = f"{safe_filename}{os.path.splitext(file.filename)[1]}"
        video_path = os.path.join(upload_dir, video_filename)

        # ë¹„ë™ê¸° íŒŒì¼ ì“°ê¸°
        content = await file.read()
        await run_in_thread(lambda: open(video_path, "wb").write(content))

        existing_keyword_path = await run_in_thread(registry.resolve, video_id)
        
        if existing_keyword_path and os.path.exists(existing_keyword_path):
            keyword_path = existing_keyword_path
            message = f"ì—…ë¡œë“œ ì™„ë£Œ ë° í‚¤ì›Œë“œ JSON ì—°ê²°ë¨: {keyword_path}"
        else:
            keyword_path = None
            message = "ì—…ë¡œë“œ ì™„ë£Œ (í‚¤ì›Œë“œ JSON ì—†ìŒ - registryì— ìˆ˜ë™ ë“±ë¡ í•„ìš”)"

        return {
            "video_id": video_id,
            "video_path": video_path,
            "keyword_path": keyword_path,
            "message": message
        }

    @app.get("/videos/{video_id:path}/stream-keywords")
    async def stream_keywords(
        video_id: str,
        current_time: float = Query(0),
        window: float = Query(5.0)
    ):
        """ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ìŠ¤íŠ¸ë¦¬ë° (Server-Sent Events)"""
        import json
        
        graph_service, keyword_store, registry = get_services()

        async def event_generator():
            keyword_path = await run_in_thread(registry.resolve, video_id)
            if not keyword_path:
                yield "data: {\"error\": \"ì˜ìƒì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\"}\n\n"
                return

            try:
                start = max(current_time - window / 2, 0)
                end = current_time + window / 2

                window_data = await run_in_thread(
                    keyword_store.query,
                    video_id,
                    start,
                    end,
                    5,
                    keyword_path
                )

                keywords = [item["term"] for item in window_data["keywords"]]

                mapped = await run_in_thread(
                    graph_service.map_keywords_to_entities,
                    keywords,
                    5
                )

                for entity in mapped:
                    entity_name = entity.get("name")
                    if entity_name:
                        neighbors = await run_in_thread(
                            graph_service.get_entity_context,
                            entity_name,
                            1
                        )

                        event_data = {
                            "time": current_time,
                            "entity": entity,
                            "neighbors": neighbors
                        }
                        data = json.dumps(event_data, ensure_ascii=False)
                        yield f"data: {data}\n\n"

            except Exception as e:
                yield f"data: {{\"error\": \"{str(e)}\"}}\n\n"

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream"
        )

    @app.get("/health")
    async def health_check():
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy",
            "workers": 4,
            "llm_queue_max_concurrent": 2
        }

    @app.get("/map-keywords")
    async def get_map_keywords():
        """ì§€ë„ íƒ€ì„ë¼ì¸ í‚¤ì›Œë“œ ì¡°íšŒ (map_keyword.json)"""
        import json
        
        keyword_root = os.environ.get("VIDEO_KEYWORD_DIR", "video_keywords")
        map_keyword_path = os.path.join(keyword_root, "map_keyword.json")
        
        if not os.path.exists(map_keyword_path):
            return {"timelineData": []}
        
        try:
            with open(map_keyword_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"map_keyword.json ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {"timelineData": []}

    @app.post("/cache/clear")
    async def clear_cache():
        """í‚¤ì›Œë“œ ìºì‹œ í´ë¦¬ì–´ (JSON ë³€ê²½ í›„ í˜¸ì¶œ)"""
        graph_service, keyword_store, registry = get_services()
        keyword_store.clear_cache()
        _preload_cache.clear()
        _preload_status.clear()
        return {"message": "ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ"}

    @app.get("/entity/{entity_name}/info")
    async def get_entity_info(entity_name: str):
        """
        ì—”í‹°í‹° ì •ë³´ ì¡°íšŒ - GraphDBì— ìˆìœ¼ë©´ ì—”í‹°í‹° ì •ë³´ ë°˜í™˜,
        ì—†ìœ¼ë©´ FAISSì—ì„œ ê´€ë ¨ ë¬¸ì„œ/ì •ë³´ ê²€ìƒ‰í•˜ì—¬ ë°˜í™˜
        """
        graph_service, keyword_store, registry = get_services()
        
        # 1. GraphDBì—ì„œ ì—”í‹°í‹° ì¡°íšŒ ì‹œë„
        entity_data = await run_in_thread(
            graph_service.get_entity_context,
            entity_name,
            1
        )
        
        if entity_data and entity_data.get("entity"):
            # GraphDBì— ìˆìŒ - ì—”í‹°í‹° ì •ë³´ì™€ sources ë°˜í™˜
            entity = entity_data["entity"]
            sources = entity.get("sources", [])
            
            return {
                "found_in": "graphdb",
                "entity_name": entity_name,
                "entity": entity,
                "sources": sources,
                "neighbors": entity_data.get("neighbors", {})
            }
        
        # 2. GraphDBì— ì—†ìŒ - FAISSì—ì„œ ê²€ìƒ‰
        print(f"[entity_info] '{entity_name}' GraphDBì— ì—†ìŒ - FAISS ê²€ìƒ‰")
        
        try:
            # FAISS ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì¡°íšŒ
            faiss_results = await run_in_thread(
                graph_service.search_documents_faiss,
                entity_name,
                5  # top_k
            )
            
            # FAISS ê²°ê³¼ì—ì„œ ê´€ë ¨ ì—”í‹°í‹°ë“¤ì˜ sources ìˆ˜ì§‘
            related_sources = []
            related_entities = []
            
            for doc in faiss_results:
                content = doc.get("content", "")[:500]
                source = doc.get("source", "")
                
                # ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                related_sources.append({
                    "doc": source.split("/")[-1].split("\\")[-1] if source else "ì•Œ ìˆ˜ ì—†ìŒ",
                    "snippet": content,
                    "type": "faiss_document"
                })
                
                # ë¬¸ì„œ ë‚´ìš©ì—ì„œ GraphDB ì—”í‹°í‹° ì°¾ê¸°
                entities_in_doc = await run_in_thread(
                    graph_service.find_entities_in_text,
                    content,
                    3  # ìµœëŒ€ 3ê°œ
                )
                
                for ent in entities_in_doc:
                    if ent.get("name") and ent["name"] not in [e.get("name") for e in related_entities]:
                        # í•´ë‹¹ ì—”í‹°í‹°ì˜ sourcesë„ ê°€ì ¸ì˜¤ê¸°
                        ent_context = await run_in_thread(
                            graph_service.get_entity_context,
                            ent["name"],
                            0
                        )
                        if ent_context and ent_context.get("entity"):
                            ent_sources = ent_context["entity"].get("sources", [])
                            related_entities.append({
                                "name": ent["name"],
                                "category": ent_context["entity"].get("category", ""),
                                "type": ent_context["entity"].get("type", ""),
                                "sources": ent_sources[:3]  # ìµœëŒ€ 3ê°œ sources
                            })
            
            return {
                "found_in": "faiss",
                "entity_name": entity_name,
                "entity": None,
                "sources": related_sources,
                "related_entities": related_entities,
                "message": f"'{entity_name}'ì€(ëŠ”) GraphDBì— ì—†ì–´ FAISSì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤."
            }
            
        except Exception as e:
            print(f"[entity_info] FAISS ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {
                "found_in": "none",
                "entity_name": entity_name,
                "entity": None,
                "sources": [],
                "message": f"'{entity_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    return app


app = create_app()


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ GraphRAG ì„œë¹„ìŠ¤ ë¯¸ë¦¬ ì´ˆê¸°í™”"""
    print(" ì„œë²„ ì‹œì‘ - GraphRAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    print("   LLM ë™ì‹œ ì²˜ë¦¬: 2ê°œ")
    
    # ë¹„ë™ê¸°ë¡œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    await run_in_thread(get_services)
    
    # í‚¤ì›Œë“œ ìŠ¤í† ì–´ ìºì‹œ í´ë¦¬ì–´ (JSON ë³€ê²½ ë°˜ì˜)
    graph_service, keyword_store, registry = get_services()
    keyword_store.clear_cache()
    print("   í‚¤ì›Œë“œ ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")
    
    # LLM í ì´ˆê¸°í™”
    get_llm_queue()
    
    print("\n" + "="*50)
    print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ - API ìš”ì²­ ëŒ€ê¸° ì¤‘")
    print("="*50 + "\n")


@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global _llm_queue, _thread_pool
    
    if _llm_queue:
        _llm_queue.shutdown()
    
    _thread_pool.shutdown(wait=False)
    print("ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    import uvicorn
    import sys

    use_reload = "--reload" in sys.argv or "-r" in sys.argv
    
    if use_reload:
        print("âš ï¸  ê°œë°œ ëª¨ë“œ (reload=True) - íŒŒì¼ ë³€ê²½ ì‹œ ì„œë²„ ì¬ì‹œì‘ë¨")
        print("   ë¬´ê±°ìš´ ëª¨ë¸ì´ ë§¤ë²ˆ ì¬ë¡œë“œë˜ë¯€ë¡œ ì£¼ì˜í•˜ì„¸ìš”!")
        # ê°œë°œ ëª¨ë“œì—ì„œëŠ” ì›Œì»¤ 1ê°œ
        uvicorn.run("api_server:app", host="0.0.0.0", port=8080, reload=True)
    else:
        # í”„ë¡œë•ì…˜ ëª¨ë“œ: ì›Œì»¤ 4ê°œ
        print("ğŸš€ í”„ë¡œë•ì…˜ ëª¨ë“œ - ì›Œì»¤ 4ê°œë¡œ ì‹¤í–‰")
        uvicorn.run(
            "api_server:app", 
            host="0.0.0.0", 
            port=8080, 
            workers=1,
            reload=False
        )
