import json
import os
import glob
import re
import hashlib
import numpy as np
from typing import List, Dict, Tuple, Optional
import torch

# LangChain imports
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
except ImportError:
    from langchain.embeddings import HuggingFaceEmbeddings

try:
    from langchain_core.documents import Document
except ImportError:
    try:
        from langchain.docstore.document import Document
    except ImportError:
        from langchain.schema import Document

try:
    from langchain_community.llms import Ollama
except ImportError:
    from langchain.llms import Ollama

# FAISS
try:
    from langchain_community.vectorstores import FAISS
except ImportError:
    from langchain.vectorstores import FAISS

# PDF ë¡œë”
try:
    from langchain_community.document_loaders import PyPDFLoader
except ImportError:
    try:
        from langchain.document_loaders import PyPDFLoader
    except ImportError:
        PyPDFLoader = None

# Tika ë¬¸ì„œ ì¶”ì¶œ
try:
    from tika import parser as tika_parser
    TIKA_AVAILABLE = True
except ImportError:
    TIKA_AVAILABLE = False

# ë¦¬ë­ì»¤ ëª¨ë¸
try:
    from sentence_transformers import CrossEncoder
    RERANKER_AVAILABLE = True
except ImportError:
    RERANKER_AVAILABLE = False

# Graph DB
from graph_db import ArangoGraphDB, KnowledgeGraphExtractor


def _normalize_metadata(
    raw_metadata: Optional[Dict],
    collection: str,
    filename: str,
    file_path: str,
    index: int
) -> Tuple[Dict, List[Dict]]:
    """í‘œì¤€ ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆì™€ ì£¼ì„ ë¶„ë¦¬"""
    metadata = {
        'source': filename,
        'file_path': file_path,
        'index': index,
        'type': 'json',
        'collection': collection,
    }

    annotations = []
    if raw_metadata and isinstance(raw_metadata, dict):
        # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „: 'í•­ëª©ëª…' ì‚¬ìš©
        title = (
            raw_metadata.get('í•­ëª©ëª…') or
            raw_metadata.get('ì œëª©') or
            raw_metadata.get('title')
        )
        author = raw_metadata.get('ì €ì') or raw_metadata.get('author')
        category = raw_metadata.get('ì¹´í…Œê³ ë¦¬') or raw_metadata.get('category')
        topic = raw_metadata.get('ì£¼ì œë¶„ë¥˜') or raw_metadata.get('topic')
        keywords = raw_metadata.get('í‚¤ì›Œë“œ') or raw_metadata.get('keywords')

        if title:
            metadata['title'] = title
        if author:
            metadata['author'] = author
        if category:
            metadata['category'] = category
        if topic:
            metadata['topic'] = topic
        if keywords:
            metadata['keywords'] = keywords

        # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì¶”ê°€ í•„ë“œ (í•œê¸€ í‚¤ ì§€ì›)
        # ì—”í‹°í‹° ìœ í˜• (ë¬¸ì„œ ìœ í˜• 'json'ê³¼ êµ¬ë¶„)
        entity_type = (
            raw_metadata.get('í•­ëª© ìœ í˜•') or
            raw_metadata.get('type')
        )
        if entity_type:
            metadata['entity_type'] = entity_type
        
        # ì›ì–´ (í•œì)
        hanja = raw_metadata.get('ì›ì–´') or raw_metadata.get('hanja')
        if hanja:
            metadata['hanja'] = hanja
        
        # ì‹œëŒ€
        era = raw_metadata.get('ì‹œëŒ€') or raw_metadata.get('era')
        if era:
            metadata['era'] = era
        
        # í•­ëª© ì •ì˜
        definition = raw_metadata.get('í•­ëª© ì •ì˜') or raw_metadata.get('definition')
        if definition:
            metadata['definition'] = definition
        
        # ìš”ì•½
        summary = raw_metadata.get('ìš”ì•½') or raw_metadata.get('summary')
        if summary:
            metadata['summary'] = summary
        
        # URL
        url = raw_metadata.get('url')
        if url:
            metadata['url'] = url
        
        # í•­ëª© ë¶„ì•¼
        field = raw_metadata.get('í•­ëª© ë¶„ì•¼') or raw_metadata.get('field')
        if field:
            metadata['category'] = field  # categoryë¡œ ë§¤í•‘

        # ğŸ†• ì˜¨í†¨ë¡œì§€ êµ¬ì¶•ìš© í•„ë“œ (ê´€ë ¨í•­ëª©, ë³¸ë¬¸ í‘œ) - í•œê¸€ í‚¤ ì§€ì›
        related = raw_metadata.get('ê´€ë ¨í•­ëª©') or raw_metadata.get('related_articles')
        if related:
            metadata['related_articles'] = related
        
        tables = raw_metadata.get('ë³¸ë¬¸ í‘œ') or raw_metadata.get('tables')
        if tables:
            metadata['tables'] = tables

        # ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì§€ì›
        annotations = raw_metadata.get('annotations') or raw_metadata.get('ì£¼ì„') or []

        # ì›ë³¸ ê°’ì„ ìƒì§€ ì•Šë„ë¡ ë³´ê´€
        metadata['raw_metadata_keys'] = list(raw_metadata.keys())

    doc_id = f"{collection}_{filename}_{index}"
    metadata['document_id'] = metadata.get('title') or doc_id

    if annotations:
        metadata['has_annotations'] = True
        metadata['annotation_ids'] = [
            ann.get('id') for ann in annotations if isinstance(ann, dict) and ann.get('id')
        ]

    return metadata, annotations


def _extract_ontology_from_encyclopedia(item: Dict) -> Dict:
    """í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ í•­ëª©ì—ì„œ ì˜¨í†¨ë¡œì§€ ì •ë³´ ì¶”ì¶œ
    
    Returns:
        {
            'entity': { ì—”í‹°í‹° ì •ë³´ },
            'relations': [ ê´€ê³„ ë¦¬ìŠ¤íŠ¸ ],
            'battle_triples': [ ì „íˆ¬ í‘œì—ì„œ ì¶”ì¶œí•œ íŠ¸ë¦¬í”Œ ]
        }
    """
    result = {
        'entity': None,
        'relations': [],
        'battle_triples': []
    }
    
    # 1. ë©”ì¸ ì—”í‹°í‹° ì¶”ì¶œ
    if item.get('í•­ëª©ëª…'):
        entity_type = item.get('í•­ëª© ìœ í˜•', '')
        # í•­ëª© ìœ í˜•ì—ì„œ ì‹¤ì œ íƒ€ì… ì¶”ì¶œ (ì˜ˆ: "ì‚¬ê±´/ì „ìŸ" â†’ "ì‚¬ê±´")
        if '/' in entity_type:
            entity_type = entity_type.split('/')[0]
        
        result['entity'] = {
            'name': item['í•­ëª©ëª…'],
            'hanja': item.get('ì›ì–´', ''),
            'type': entity_type,
            'category': item.get('í•­ëª© ë¶„ì•¼', ''),
            'era': item.get('ì‹œëŒ€', ''),
            'definition': item.get('í•­ëª© ì •ì˜', ''),
            'summary': item.get('ìš”ì•½', ''),
            'url': item.get('url', ''),
            'source': 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „'
        }
    
    # 2. ê´€ë ¨í•­ëª©ì—ì„œ ê´€ê³„ ì¶”ì¶œ
    related_articles = item.get('ê´€ë ¨í•­ëª©', [])
    main_name = item.get('í•­ëª©ëª…', '')
    
    for related in related_articles:
        if not isinstance(related, dict):
            continue
        
        related_name = related.get('í•­ëª©ëª…', '')
        if not related_name:
            continue
        
        related_type = related.get('í•­ëª© ìœ í˜•', '')
        if '/' in related_type:
            related_type = related_type.split('/')[0]
        
        # ê´€ê³„ íƒ€ì… ì¶”ë¡ 
        relation_type = 'ê´€ë ¨_í•­ëª©'
        if 'ì¸ë¬¼' in related_type:
            relation_type = 'ê´€ë ¨_ì¸ë¬¼'
        elif 'ì‚¬ê±´' in related_type or 'ì „ìŸ' in related_type:
            relation_type = 'ê´€ë ¨_ì‚¬ê±´'
        elif 'ì¥ì†Œ' in related_type or 'ì§€ë¦¬' in related_type:
            relation_type = 'ê´€ë ¨_ì¥ì†Œ'
        elif 'ì‘í’ˆ' in related_type or 'ë¬¸í•™' in related_type:
            relation_type = 'ê´€ë ¨_ë¬¸í—Œ'
        
        result['relations'].append({
            'subject': main_name,
            'subject_type': result['entity']['type'] if result['entity'] else '',
            'predicate': relation_type,
            'object': related_name,
            'object_type': related_type,
            'object_hanja': related.get('ì›ì–´', ''),
            'object_url': related.get('URL', ''),
            'object_definition': related.get('í•­ëª© ì •ì˜', ''),
            'source': 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „'
        })
    
    # 3. ë³¸ë¬¸ í‘œì—ì„œ ì „íˆ¬ íŠ¸ë¦¬í”Œ ì¶”ì¶œ (ì„ì§„ì™œë€ ëŒ€ì†Œì „íˆ¬ ë“±)
    tables = item.get('ë³¸ë¬¸ í‘œ', [])
    for table in tables:
        if not isinstance(table, dict):
            continue
        
        title = table.get('title', '')
        rows = table.get('rows', [])
        
        # ì „íˆ¬ í‘œì¸ ê²½ìš°
        if 'ì „íˆ¬' in title or 'ëŒ€ì²©' in title:
            for row in rows:
                if not isinstance(row, dict):
                    continue
                
                # ë‚ ì§œ, ì¥ì†Œ, ì¡°ì„ ì¸¡, ì™œì¸¡, ê²°ê³¼ ì¶”ì¶œ
                date = row.get('col_0', '')
                place = row.get('col_1', '')
                joseon_commander = row.get('col_2', '')
                japan_commander = row.get('col_3', '')
                outcome = row.get('col_4', '')
                
                if place and joseon_commander:
                    # ì¥ì†Œ ì—”í‹°í‹°ì™€ ì „íˆ¬ ê´€ê³„
                    battle_name = f"{place} ì „íˆ¬" if 'ì „íˆ¬' not in place and 'ëŒ€ì²©' not in place else place
                    
                    # ì¡°ì„  ì§€íœ˜ê´€ â†’ ì „íˆ¬ ì°¸ì—¬
                    result['battle_triples'].append({
                        'subject': joseon_commander.replace('(', '').replace(')', ''),
                        'subject_type': 'ì¸ë¬¼',
                        'predicate': 'ì „íˆ¬_ì°¸ì—¬',
                        'object': battle_name,
                        'object_type': 'ì „íˆ¬',
                        'date': date,
                        'outcome': outcome,
                        'side': 'ì¡°ì„ ',
                        'source': main_name
                    })
                    
                    # ì¼ë³¸ ì§€íœ˜ê´€ â†’ ì „íˆ¬ ì°¸ì—¬
                    if japan_commander and japan_commander != '?':
                        result['battle_triples'].append({
                            'subject': japan_commander,
                            'subject_type': 'ì¸ë¬¼',
                            'predicate': 'ì „íˆ¬_ì°¸ì—¬',
                            'object': battle_name,
                            'object_type': 'ì „íˆ¬',
                            'date': date,
                            'outcome': outcome,
                            'side': 'ì¼ë³¸',
                            'source': main_name
                        })
                    
                    # ì „íˆ¬ â†’ ì¥ì†Œ
                    result['battle_triples'].append({
                        'subject': battle_name,
                        'subject_type': 'ì „íˆ¬',
                        'predicate': 'ë°œìƒ_ì¥ì†Œ',
                        'object': place,
                        'object_type': 'ì¥ì†Œ',
                        'date': date,
                        'source': main_name
                    })
    
    return result


def _append_annotations_to_content(content: str, annotations: List[Dict]) -> str:
    if not annotations:
        return content

    annotation_lines = []
    for ann in annotations:
        if not isinstance(ann, dict):
            continue
        label = ann.get('label') or ann.get('id') or 'annotation'
        text = ann.get('text') or ''
        if not text:
            continue
        annotation_lines.append(f"- [{label}] {text}")

    if not annotation_lines:
        return content

    return f"{content.rstrip()}\n\n### ì£¼ì„\n" + "\n".join(annotation_lines)


def _build_annotation_documents(
    annotations: List[Dict],
    base_metadata: Dict
) -> List[Dict]:
    annotation_docs = []
    for idx, ann in enumerate(annotations):
        if not isinstance(ann, dict):
            continue
        text = ann.get('text')
        if not text:
            continue

        ann_meta = dict(base_metadata)
        ann_meta.update({
            'type': 'annotation',
            'annotation_id': ann.get('id') or f"{base_metadata.get('document_id')}_ann_{idx}",
            'annotation_label': ann.get('label'),
            'annotation_index': idx,
        })

        annotation_docs.append({
            'content': text,
            'metadata': ann_meta
        })
    return annotation_docs


def load_documents_from_output(output_dir='output'):
    """output ë””ë ‰í† ë¦¬ì—ì„œ JSON ë° PDF ë¬¸ì„œ ë¡œë“œ
    
    Args:
        output_dir: ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\n{output_dir} ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
    
    if not os.path.exists(output_dir):
        print(f"ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {output_dir}")
        return []
    
    documents = []
    
    # JSON íŒŒì¼ ë¡œë“œ
    json_files = glob.glob(os.path.join(output_dir, '**/*.json'), recursive=True)
    print(f"JSON íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬")
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            filename = os.path.basename(json_file)
            collection_name = os.path.basename(os.path.dirname(json_file))
            
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ JSON
            if isinstance(data, list):
                for idx, item in enumerate(data):
                    # ë‹¤ì–‘í•œ content í•„ë“œëª… ì§€ì›
                    content = (
                        item.get('content') or 
                        item.get('í•­ëª© ë³¸ë¬¸') or  # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „
                        item.get('body') or
                        ''
                    )
                    if content:
                        # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ í˜•ì‹ì¸ ê²½ìš° ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        if 'í•­ëª©ëª…' in item:
                            raw_metadata = {
                                'title': item.get('í•­ëª©ëª…'),
                                'url': item.get('url'),
                                'category': item.get('í•­ëª© ë¶„ì•¼'),
                                'type': item.get('í•­ëª© ìœ í˜•'),
                                'era': item.get('ì‹œëŒ€'),
                                'definition': item.get('í•­ëª© ì •ì˜'),
                                'summary': item.get('ìš”ì•½'),
                                'keywords': item.get('í‚¤ì›Œë“œ'),
                                'hanja': item.get('ì›ì–´'),
                                'annotations': item.get('ì£¼ì„', []),
                                # ğŸ†• ì˜¨í†¨ë¡œì§€ êµ¬ì¶•ìš© ì¶”ê°€ í•„ë“œ
                                'related_articles': item.get('ê´€ë ¨í•­ëª©', []),
                                'tables': item.get('ë³¸ë¬¸ í‘œ', []),
                            }
                        else:
                            raw_metadata = item.get('metadata', {})
                        
                        normalized_meta, annotations = _normalize_metadata(
                            raw_metadata,
                            collection_name,
                            filename,
                            json_file,
                            idx
                        )
                        
                        # URL ì¶”ê°€
                        if item.get('url'):
                            normalized_meta['url'] = item.get('url')
                        
                        content_with_notes = _append_annotations_to_content(content, annotations)
                        documents.append({
                            'content': content_with_notes,
                            'metadata': normalized_meta
                        })
                        if annotations:
                            documents.extend(_build_annotation_documents(annotations, normalized_meta))
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ JSON
            elif isinstance(data, dict):
                if 'documents' in data:
                    for idx, item in enumerate(data['documents']):
                        content = item.get('content', '')
                        if content:
                            normalized_meta, annotations = _normalize_metadata(
                                item.get('metadata', {}),
                                collection_name,
                                filename,
                                json_file,
                                idx
                            )
                            if item.get('title') and 'title' not in normalized_meta:
                                normalized_meta['title'] = item['title']
                            content_with_notes = _append_annotations_to_content(content, annotations)
                            documents.append({
                                'content': content_with_notes,
                                'metadata': normalized_meta
                            })
                            if annotations:
                                documents.extend(_build_annotation_documents(annotations, normalized_meta))
            
            print(f"  âœ“ {filename}")
            
        except Exception as e:
            print(f"  âœ— {filename}: {e}")
    
    # PDF íŒŒì¼ ë¡œë“œ
    if PyPDFLoader:
        pdf_files = glob.glob(os.path.join(output_dir, '**/*.pdf'), recursive=True)
        print(f"\nPDF íŒŒì¼ {len(pdf_files)}ê°œ ë°œê²¬")
        
        for pdf_file in pdf_files:
            try:
                filename = os.path.basename(pdf_file)
                loader = PyPDFLoader(pdf_file)
                pdf_docs = loader.load()
                
                for page_num, doc in enumerate(pdf_docs):
                    if doc.page_content.strip():
                        collection_name = os.path.basename(os.path.dirname(pdf_file))
                        documents.append({
                            'content': doc.page_content,
                            'metadata': {
                                'source': filename,
                                'file_path': pdf_file,
                                'page': page_num + 1,
                                'type': 'pdf',
                                'collection': collection_name
                            }
                        })
                
                print(f"  âœ“ {filename} ({len(pdf_docs)}í˜ì´ì§€)")
                
            except Exception as e:
                print(f"  âœ— {filename}: {e}")
    else:
        print("\nPyPDFLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("ì„¤ì¹˜: pip install pypdf")
    
    print(f"\nì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    return documents


class GraphRAGSystem:
    """ê·¸ë˜í”„ + ë²¡í„° ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ (ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨)"""
    
    def __init__(
        self,
        embedding_model_name='jhgan/ko-sroberta-multitask',
        llm_model_name='gemma3:12b',
        arango_host='localhost',
        arango_port=8529,
        arango_password='',
        arango_db_name='knowledge_graph',
        arango_reset=False,
        global_arango_db_name=None,
        global_arango_reset=False,
        use_reranker=True,
        reranker_model='BAAI/bge-reranker-v2-m3',
        use_tika=False
    ):
        """ì´ˆê¸°í™”
        
        Args:
            embedding_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            llm_model_name: LLM ëª¨ë¸ ì´ë¦„
            arango_host: ArangoDB í˜¸ìŠ¤íŠ¸
            arango_port: ArangoDB í¬íŠ¸
            arango_password: ArangoDB ë¹„ë°€ë²ˆí˜¸
            arango_reset: Trueë©´ ArangoDB ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
            reranker_model: ë¦¬ë­ì»¤ ëª¨ë¸ ì´ë¦„
            use_tika: Apache Tika ì‚¬ìš© ì—¬ë¶€
        """
        print("\nê³ ê¸‰ GraphRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # GPU/CPU ìë™ ê°ì§€
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ë””ë°”ì´ìŠ¤: {device.upper()}")
        if device == 'cuda':
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
        
        # ì„ë² ë”© ëª¨ë¸
        print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {embedding_model_name}")
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name=embedding_model_name,
                model_kwargs={'device': device},  # GPU/CPU ìë™ ì„ íƒ
                encode_kwargs={'normalize_embeddings': True}
            )
            print(f"  âœ“ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ ({device.upper()})")
        except Exception as e:
            print(f"  âœ— ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("  ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            self.embeddings = HuggingFaceEmbeddings(
                model_name='jhgan/ko-sroberta-multitask',
                model_kwargs={'device': device}
            )
        
        # LLM
        print(f"LLM ì´ˆê¸°í™”: {llm_model_name}")
        try:
            self.llm = Ollama(
                model=llm_model_name,
                temperature=0.7
            )
            print("  âœ“ LLM ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"  âœ— LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm = None
        
        # ë¦¬ë­ì»¤ ì´ˆê¸°í™”
        self.use_reranker = use_reranker
        self.reranker = None
        if use_reranker and RERANKER_AVAILABLE:
            self._init_reranker(reranker_model)
        elif use_reranker and not RERANKER_AVAILABLE:
            print("\nâš ï¸  ë¦¬ë­ì»¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print("   ì„¤ì¹˜: pip install sentence-transformers")
            self.use_reranker = False
        
        # Tika ì„¤ì •
        self.use_tika = use_tika
        if use_tika and not TIKA_AVAILABLE:
            print("\nâš ï¸  Tikaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print("   ì„¤ì¹˜: pip install tika")
            self.use_tika = False
        
        # ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤
        self.graph_db = ArangoGraphDB(
            host=arango_host,
            port=arango_port,
            password=arango_password,
            db_name=arango_db_name,
            reset=arango_reset
        )
        self.global_graph_db = None
        if global_arango_db_name:
            if global_arango_db_name == arango_db_name and not global_arango_reset and arango_reset:
                # ë™ì¼ DBë¥¼ ë‘ ë²ˆ ì´ˆê¸°í™”í•˜ì§€ ì•Šë„ë¡ reset ìš°ì„ ìˆœìœ„ ì¡°ì •
                global_arango_reset = False
            self.global_graph_db = ArangoGraphDB(
                host=arango_host,
                port=arango_port,
                password=arango_password,
                db_name=global_arango_db_name,
                reset=global_arango_reset
            )
        
        # ë²¡í„° ìŠ¤í† ì–´
        self.vectorstore = None
        self.entity_vectorstore = None
        
        # ì—”í‹°í‹° ì„ë² ë”© ìºì‹œ
        self.entity_embeddings = {}  # entity_name -> embedding vector
        
        print("\nâœ“ GraphRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        if self.use_reranker:
            print("  - ë¦¬ë­ì»¤: í™œì„±í™”")
        if self.use_tika:
            print("  - Tika: í™œì„±í™”")
    
    def _init_reranker(self, model_name: str):
        """ë¦¬ë­ì»¤ ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"\në¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ: {model_name}")
        try:
            self.reranker = CrossEncoder(model_name, max_length=512)
            print("  âœ“ ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"  âœ— ë¦¬ë­ì»¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.reranker = None
            self.use_reranker = False
    
    def build_index(
        self,
        documents: List[Dict],
        extract_graph: bool = True,
        skip_vector_index: bool = False
    ):
        """ì¸ë±ìŠ¤ êµ¬ì¶• (ë²¡í„° + ê·¸ë˜í”„)
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            extract_graph: ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ ì—¬ë¶€
            skip_vector_index: ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ê±´ë„ˆë›°ê¸° (êµ¬ì¡°í™”ëœ ë°ì´í„°ìš©)
        """
        print(f"\nGraphRAG ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘ ({len(documents)}ê°œ ë¬¸ì„œ)")
        
        # 1. ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• (ë¬¸ì„œ ê¸°ë°˜) - ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ
        if skip_vector_index:
            print("\n[1/3] ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ê±´ë„ˆëœ€ (êµ¬ì¡°í™”ëœ ë°ì´í„°)")
        else:
        print("\n[1/3] ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self._build_vector_index(documents)
        
        # 2. ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ ë° ì €ì¥
        if extract_graph and self.graph_db.db:
            print("\n[2/3] ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ ì¤‘...")
            self._build_knowledge_graph(documents)
        
        # 3. ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        print("\n[3/3] ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self._build_entity_vector_index()
        
        print("\nGraphRAG ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    
    def _build_vector_index(self, documents: List[Dict]):
        """ë¬¸ì„œ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        # Document ê°ì²´ë¡œ ë³€í™˜
        docs = []
        for doc in documents:
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})
            
            if content:
                docs.append(Document(
                    page_content=content,
                    metadata=metadata
                ))
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        split_docs = text_splitter.split_documents(docs)
        
        print(f"  {len(split_docs)}ê°œ ì²­í¬ ìƒì„±")
        
        # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = FAISS.from_documents(
            documents=split_docs,
            embedding=self.embeddings
        )
        
        print("  ë¬¸ì„œ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def _build_knowledge_graph(self, documents: List[Dict]):
        """ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• (ì˜¨í†¨ë¡œì§€ + LLM í•˜ì´ë¸Œë¦¬ë“œ)"""
        all_entities = []
        all_relations = []
        
        # 1ë‹¨ê³„: ë¬¸ì„œ ë¶„ë¥˜ (í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ vs ì¼ë°˜ ì‚¬ë£Œ)
        encyclopedia_docs = []
        other_docs = []
        
        for doc in documents:
            metadata = doc.get('metadata', {})
            collection = metadata.get('collection', '')
            
            # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ í˜•ì‹ í™•ì¸ (ë‹¤ì–‘í•œ ì¡°ê±´)
            is_encyclopedia = (
                'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „' in collection or
                'encykorea' in collection.lower() or
                metadata.get('hanja') or  # ì›ì–´(í•œì)ê°€ ìˆìœ¼ë©´
                metadata.get('era') or    # ì‹œëŒ€ ì •ë³´ê°€ ìˆìœ¼ë©´
                metadata.get('definition') or  # í•­ëª© ì •ì˜ê°€ ìˆìœ¼ë©´
                metadata.get('related_articles') or
                metadata.get('tables')
            )
            
            if is_encyclopedia:
                encyclopedia_docs.append(doc)
            else:
                other_docs.append(doc)
        
        # 2ë‹¨ê³„: í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì˜¨í†¨ë¡œì§€ êµ¬ì¶• (LLM ë¶ˆí•„ìš”, ë¹ ë¦„)
        if encyclopedia_docs:
            print(f"  ğŸ“š í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì˜¨í†¨ë¡œì§€ êµ¬ì¶• ì¤‘ ({len(encyclopedia_docs)}ê°œ í•­ëª©)...")
            onto_entities, onto_relations = self._extract_encyclopedia_ontology(encyclopedia_docs)
            all_entities.extend(onto_entities)
            all_relations.extend(onto_relations)
            print(f"    â†’ ì—”í‹°í‹° {len(onto_entities)}ê°œ, ê´€ê³„ {len(onto_relations)}ê°œ ì¶”ì¶œ")
        
        # 3ë‹¨ê³„: ë‚˜ë¨¸ì§€ ë¬¸ì„œì—ì„œ LLM ê¸°ë°˜ ì¶”ì¶œ (ê°„ì–‘ë¡, ë‚œì¤‘ì¼ê¸° ë“± ì¼ë°˜ ì‚¬ë£Œ)
        if other_docs:
            print(f"  ğŸ¤– LLM ê¸°ë°˜ ì§€ì‹ ì¶”ì¶œ ì¤‘ ({len(other_docs)}ê°œ ë¬¸ì„œ)...")
        extractor = KnowledgeGraphExtractor(
            llm_model=self.llm.model if self.llm else 'deepseek-r1:latest'
        )
        llm_entities, llm_relations = extractor.extract_entities_and_relations(other_docs)
        all_entities.extend(llm_entities)
        all_relations.extend(llm_relations)
        print(f"    â†’ ì—”í‹°í‹° {len(llm_entities)}ê°œ, ê´€ê³„ {len(llm_relations)}ê°œ ì¶”ì¶œ")
        
        # 4ë‹¨ê³„: ê·¸ë˜í”„ DBì— ì‚½ì…
        self._insert_into_graphs(all_entities, all_relations)
    
    def _extract_encyclopedia_ontology(self, docs: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ì—ì„œ ì˜¨í†¨ë¡œì§€ ì§ì ‘ ì¶”ì¶œ (LLM ë¶ˆí•„ìš”)
        
        ì¶”ì¶œ ì†ŒìŠ¤:
        1. ë©”ì¸ ì—”í‹°í‹° (í•­ëª©ëª…, ì •ì˜, ì‹œëŒ€ ë“±)
        2. ê´€ë ¨í•­ëª© â†’ ëª…ì‹œì  ê´€ê³„
        """
        entities = []
        relations = []
        seen_entities = set()
        
        # ë³¸ë¬¸ ë‚´ ë§í¬ íŒ¨í„´: [í‘œì‹œí…ìŠ¤íŠ¸](E0000000) ë˜ëŠ” [ã€ì±…ì´ë¦„ã€](ID)
        import re
        link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
        
        
        # ë””ë²„ê¹…: title ì—†ëŠ” ë¬¸ì„œ ìˆ˜ ì¶”ì 
        no_title_count = 0
        
        for doc in docs:
            metadata = doc.get('metadata', {})
            
            # ë©”ì¸ ì—”í‹°í‹° ìƒì„±
            title = metadata.get('title')
            if not title:
                no_title_count += 1
                if no_title_count <= 5:
                    # ë””ë²„ê¹…: metadataì˜ í‚¤ í™•ì¸
                    keys = list(metadata.keys())[:10]
                    print(f"    [DEBUG] title ì—†ìŒ - metadata keys: {keys}")
                continue
            
            # entity_type ì‚¬ìš© (ë¬¸ì„œ type='json'ê³¼ êµ¬ë¶„)
            entity_type = metadata.get('entity_type') or metadata.get('category') or 'í•­ëª©'
            if '/' in str(entity_type):
                entity_type = entity_type.split('/')[0]
            
            # category(í•­ëª© ë¶„ì•¼)ëŠ” í•­ìƒ ì¡´ì¬í•˜ë¯€ë¡œ í‚¤ ìƒì„±ì— ì‚¬ìš©
            category = metadata.get('category', '')
            
            # ì¤‘ë³µ ì²´í¬: name + category ì¡°í•© (ë™ìŒì´ì˜ì–´ êµ¬ë¶„)
            entity_key = f"{title}_{category}" if category else f"{title}_{entity_type}"
            if entity_key in seen_entities:
                continue
            
            # í•­ëª© ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            content = doc.get('content', '')
            
            entities.append({
                'name': title,
                'type': entity_type,
                'hanja': metadata.get('hanja', ''),
                'era': metadata.get('era', ''),
                'category': category,
                'definition': metadata.get('definition', ''),
                'summary': metadata.get('summary', ''),
                'content': content,  # í•­ëª© ë³¸ë¬¸ ì €ì¥
                'url': metadata.get('url', ''),  # ëŒ€í‘œ URL (í˜¸í™˜ì„±)
                'sources': [{
                    'type': 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „',
                    'doc': 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „',  # ì‚¬ë£Œëª…
                    'title': title,  # í•­ëª©ëª…
                    'url': metadata.get('url', ''),  # ì†ŒìŠ¤ë³„ URL
                    'snippet': metadata.get('definition', '')[:200]
                }]
            })
            seen_entities.add(entity_key)
            
            # ê´€ë ¨í•­ëª©ì—ì„œ ê´€ê³„ë§Œ ì¶”ì¶œ (ì—”í‹°í‹°ëŠ” ë©”ì¸ ë¬¸ì„œì—ì„œë§Œ ìƒì„±)
            related_articles = metadata.get('related_articles', [])
            for related in related_articles:
                if not isinstance(related, dict):
                    continue
                
                related_name = related.get('í•­ëª©ëª…', '')
                if not related_name:
                    continue
                
                # ê´€ê³„ íƒ€ì… ì¶”ë¡ 
                related_type = related.get('í•­ëª© ìœ í˜•', '')
                predicate = 'ê´€ë ¨_í•­ëª©'
                if 'ì¸ë¬¼' in related_type:
                    predicate = 'ê´€ë ¨_ì¸ë¬¼'
                elif 'ì‚¬ê±´' in related_type or 'ì „ìŸ' in related_type:
                    predicate = 'ê´€ë ¨_ì‚¬ê±´'
                elif 'ì¥ì†Œ' in related_type or 'ì§€ë¦¬' in related_type:
                    predicate = 'ê´€ë ¨_ì¥ì†Œ'
                elif 'ì‘í’ˆ' in related_type or 'ë¬¸í•™' in related_type:
                    predicate = 'ê´€ë ¨_ë¬¸í—Œ'
                
                # ë™ëª…ì´ì˜ì–´ ë§¤ì¹­ìš© ì¶”ê°€ ì •ë³´ í¬í•¨
                # subject_fieldëŠ” ë©”ì¸ ì—”í‹°í‹°ì˜ category(í•­ëª© ë¶„ì•¼)
                relations.append({
                    'subject': title,
                    'subject_type': entity_type,
                    'subject_hanja': metadata.get('hanja', ''),
                    'subject_field': metadata.get('category', ''),
                    'predicate': predicate,
                    'object': related_name,
                    'object_type': related_type.split('/')[0] if related_type else '',
                    'object_hanja': related.get('ì›ì–´', ''),
                    'object_field': related.get('í•­ëª© ë¶„ì•¼', ''),
                    'source': 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „'
                })
            
            # ë³¸ë¬¸ ë‚´ ë§í¬ì—ì„œ ê´€ê³„ë§Œ ì¶”ì¶œ (ì—”í‹°í‹°ëŠ” ë©”ì¸ ë¬¸ì„œì—ì„œë§Œ ìƒì„±)
            content = doc.get('content', '')
            if content and title:
                links = link_pattern.findall(content)
                seen_links = set()  # ë™ì¼ ë¬¸ì„œ ë‚´ ì¤‘ë³µ ë§í¬ ë°©ì§€
                for display_text, link_id in links:
                    # ë™ì¼ link_idëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                    if link_id in seen_links:
                        continue
                    seen_links.add(link_id)
                    
                    # í‘œì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ëª… ì¶”ì¶œ
                    entity_name = display_text.strip()
                    # ê´„í˜¸ ì•ˆì˜ í•œì ì œê±°
                    if '(' in entity_name:
                        entity_name = entity_name.split('(')[0].strip()
                    # ã€ã€ ì œê±°
                    entity_name = entity_name.replace('ã€', '').replace('ã€', '')
                    entity_name = entity_name.replace('[', '').replace(']', '')
                    
                    if not entity_name or len(entity_name) < 2:
                        continue
                    if entity_name == title:  # ìê¸° ìì‹  ì°¸ì¡° ì œì™¸
                        continue
                    
                    # ê´€ê³„ ì¶”ê°€ (link_idë¡œ ì •í™•í•œ ì—”í‹°í‹° ë§¤ì¹­ ê°€ëŠ¥)
                    relations.append({
                        'subject': title,
                        'subject_type': entity_type,
                        'subject_hanja': metadata.get('hanja', ''),
                        'subject_field': category,
                        'predicate': 'ë³¸ë¬¸_ì–¸ê¸‰',
                        'object': entity_name,
                        'object_type': '',  # ë³¸ë¬¸ ë§í¬ì—ëŠ” íƒ€ì… ì •ë³´ ì—†ìŒ
                        'object_hanja': '',
                        'object_field': '',
                        'object_url_id': link_id,  # URL IDë¡œ ì •í™•í•œ ë§¤ì¹­ ê°€ëŠ¥
                        'source': 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „'
                    })
        
        return entities, relations
    
    def _insert_into_graphs(self, entities: List[Dict], relations: List[Dict]):
        targets = []
        if self.graph_db and self.graph_db.db:
            targets.append(('ê°œë³„ ê·¸ë˜í”„', self.graph_db))
        if self.global_graph_db and self.global_graph_db.db:
            targets.append(('í†µí•© ê·¸ë˜í”„', self.global_graph_db))
        
        if not targets:
            return
        
        # ì—”í‹°í‹° ì´ë¦„ â†’ í‚¤ ë§¤í•‘ ìƒì„± (ê´€ê³„ ì‚½ì…ìš©)
        # category(í•­ëª© ë¶„ì•¼)ê°€ í•­ìƒ ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ í‚¤ ìƒì„±ì— ì‚¬ìš©
        entity_key_map = {}
        for entity in entities:
            name = entity.get('name', '')
            if name:
                # _keyê°€ ì—†ìœ¼ë©´ ìƒì„± (category ìš°ì„ , ì—†ìœ¼ë©´ type ì‚¬ìš©)
                category = entity.get('category', '')
                entity_type = entity.get('type', '')
                if category:
                    key = self._sanitize_key_for_entity(f"{name}_{category}")
                elif entity_type and entity_type != 'ë¯¸ë¶„ë¥˜':
                    key = self._sanitize_key_for_entity(f"{name}_{entity_type}")
                else:
                    key = self._sanitize_key_for_entity(name)
                if name not in entity_key_map:
                    entity_key_map[name] = {}
                # ë§¤í•‘ í‚¤ë„ category ê¸°ë°˜ìœ¼ë¡œ
                map_key = category or entity_type or ''
                entity_key_map[name][map_key] = key
        
        print(f"  â†’ ì—”í‹°í‹° ë§¤í•‘: {len(entity_key_map)}ê°œ")
        
        for label, db in targets:
            db.insert_entities(entities)
            # ì—”í‹°í‹° ì‚½ì… í›„, ì‹¤ì œ DBì—ì„œ ë§¤í•‘ ë¡œë“œ (ë³‘í•©ëœ í‚¤ ë°˜ì˜)
            # entity_key_mapì€ ì˜ˆìƒ í‚¤, ì‹¤ì œ DBì˜ í‚¤ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            db.insert_relations(relations, entity_key_map=None)  # DBì—ì„œ ë¡œë“œí•˜ë„ë¡
            stats = db.get_statistics()
            print(f"  [{label}] ì—”í‹°í‹°: {stats.get('entities_count', 0)}ê°œ, ê´€ê³„: {stats.get('relations_count', 0)}ê°œ")
    
    def _sanitize_key_for_entity(self, text: str) -> str:
        """ì—”í‹°í‹° í‚¤ ìƒì„± (graph_db.pyì™€ ë™ì¼í•œ ë¡œì§ - SHA256 24ì)"""
        import hashlib
        import re
        if not text or not isinstance(text, str):
            return 'unknown_' + hashlib.sha256(str(id(text)).encode()).hexdigest()[:8]
        normalized = text.replace(' ', '_')
        ascii_only = re.sub(r'[^a-zA-Z0-9_-]', '', normalized)
        # ì‹¤ì œ ì˜ìˆ«ìê°€ 3ì ì´ìƒì¸ ê²½ìš°ë§Œ ASCII í‚¤ ì‚¬ìš©
        alphanumeric_only = re.sub(r'[^a-zA-Z0-9]', '', ascii_only)
        if alphanumeric_only and len(alphanumeric_only) >= 3:
            if not ascii_only[0].isalpha():
                ascii_only = 'K_' + ascii_only
            return ascii_only[:128]
        # SHA256 í•´ì‹œì˜ ì• 24ì ì‚¬ìš© (ì¶©ëŒ í™•ë¥  ê·¹íˆ ë‚®ìŒ)
        hash_part = hashlib.sha256(text.encode('utf-8')).hexdigest()[:24]
        return f"K_{hash_part}"
    
    def _build_entity_vector_index(self):
        """ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• (KNN ê²€ìƒ‰ìš©)"""
        if not self.graph_db.db:
            print("  ê·¸ë˜í”„ DBê°€ ì—†ì–´ ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        try:
            # ëª¨ë“  ì—”í‹°í‹° ê°€ì ¸ì˜¤ê¸°
            entities_collection = self.graph_db.db.collection('entities')
            entities = list(entities_collection.all())
            
            if not entities:
                print("  ì—”í‹°í‹°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print(f"  {len(entities)}ê°œ ì—”í‹°í‹° ì„ë² ë”© ì¤‘...")
            
            # ì—”í‹°í‹°ë¥¼ Documentë¡œ ë³€í™˜
            entity_docs = []
            for entity in entities:
                name = entity.get('name', '')
                entity_type = entity.get('type', 'entity')
                sources = entity.get('sources', [])
                
                # ì—”í‹°í‹° ì„¤ëª… ìƒì„±
                description = f"{name} (íƒ€ì…: {entity_type})"
                if sources:
                    doc_names = []
                    for src in sources[:3]:
                        if isinstance(src, dict):
                            doc_names.append(src.get('doc') or src.get('type') or 'unknown')
                        else:
                            doc_names.append(str(src))
                    description += f" [ì¶œì²˜: {', '.join(doc_names)}]"
                
                entity_docs.append(Document(
                    page_content=description,
                    metadata={
                        'entity_name': name,
                        'entity_key': entity['_key'],
                        'type': entity_type,
                        'is_entity': True
                    }
                ))
            
            # ì—”í‹°í‹° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            self.entity_vectorstore = FAISS.from_documents(
                documents=entity_docs,
                embedding=self.embeddings
            )
            
            # ì—”í‹°í‹° ì„ë² ë”© ìºì‹œ ìƒì„±
            for doc, entity in zip(entity_docs, entities):
                embedding = self.embeddings.embed_query(doc.page_content)
                self.entity_embeddings[entity['name']] = np.array(embedding)
            
            print(f"  ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"  ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì˜¤ë¥˜: {e}")
    
    def search_entities_knn(
        self,
        query: str,
        k: int = 5
    ) -> List[Dict]:
        """ì—”í‹°í‹° KNN ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ì—”í‹°í‹° ìˆ˜
            
        Returns:
            ìœ ì‚¬í•œ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        """
        if not self.entity_vectorstore:
            print("ì—”í‹°í‹° ë²¡í„° ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"\nì—”í‹°í‹° KNN ê²€ìƒ‰: '{query}' (k={k})")
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.entity_vectorstore.similarity_search_with_score(query, k=k)
        
        # ê²°ê³¼ ì •ë¦¬
        entities = []
        for doc, score in results:
            entity_name = doc.metadata.get('entity_name', '')
            similarity = float(1 / (1 + score))
            
            entities.append({
                'name': entity_name,
                'type': doc.metadata.get('type', 'entity'),
                'similarity_score': similarity,
                'description': doc.page_content
            })
        
        return entities
    
    def search_documents(
        self,
        query: str,
        k: int = 5,
        use_reranker: bool = None
    ) -> List[Dict]:
        """ë¬¸ì„œ ë²¡í„° ê²€ìƒ‰ (ë¦¬ë­í‚¹ ì˜µì…˜)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ë”°ë¦„)
            
        Returns:
            ìœ ì‚¬í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            print("ë¬¸ì„œ ë²¡í„° ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"\në¬¸ì„œ ë²¡í„° ê²€ìƒ‰: '{query}' (k={k})")
        
        # ë¦¬ë­í‚¹ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        if use_reranker is None:
            use_reranker = self.use_reranker
        
        # ì´ˆê¸° ê²€ìƒ‰ (ë¦¬ë­í‚¹ ì‹œ ë” ë§ì´ ê°€ì ¸ì˜´)
        initial_k = k * 3 if use_reranker and self.reranker else k
        results = self.vectorstore.similarity_search_with_score(query, k=initial_k)
        
        print(f"  1ë‹¨ê³„: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰")
        
        # ë¦¬ë­í‚¹ ìˆ˜í–‰
        if use_reranker and self.reranker and len(results) > 0:
            results = self._rerank_results(query, results, k)
            print(f"  2ë‹¨ê³„: ë¦¬ë­í‚¹ ì™„ë£Œ, ìƒìœ„ {len(results)}ê°œ ì„ íƒ")
        
        documents = []
        for doc, score in results[:k]:
            documents.append({
                'content': doc.page_content,
                'metadata': doc.metadata,
                'similarity_score': float(1 / (1 + score))
            })
        
        return documents
    
    def _rerank_results(self, query: str, results: list, top_k: int) -> list:
        """ë¦¬ë­ì»¤ë¡œ ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            results: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ [(doc, score), ...]
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ì¬ìˆœìœ„í™”ëœ ê²°ê³¼ [(doc, rerank_score), ...]
        """
        if not self.reranker or len(results) == 0:
            return results
        
        try:
            # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
            query_doc_pairs = [[query, doc.page_content[:512]] for doc, _ in results]
            
            # ë¦¬ë­í‚¹ ì ìˆ˜ ê³„ì‚°
            rerank_scores = self.reranker.predict(query_doc_pairs)
            
            # (ë¬¸ì„œ, ë¦¬ë­í‚¹ ì ìˆ˜) ìŒìœ¼ë¡œ ì¬êµ¬ì„±
            reranked = [(results[i][0], float(rerank_scores[i])) 
                        for i in range(len(results))]
            
            # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë†’ì€ ì ìˆ˜ê°€ ë” ê´€ë ¨ì„± ë†’ìŒ)
            reranked.sort(key=lambda x: x[1], reverse=True)
            
            return reranked[:top_k]
            
        except Exception as e:
            print(f"  âš ï¸  ë¦¬ë­í‚¹ ì˜¤ë¥˜: {e}")
            return results[:top_k]
    
    def _document_signature(self, doc: Dict) -> str:
        metadata = doc.get('metadata') or {}
        source = str(metadata.get('source') or metadata.get('file_path') or '')
        locator = str(metadata.get('page') or metadata.get('index') or metadata.get('chunk_id') or '')
        if not source and not locator:
            snippet = doc.get('content', '')[:80]
            return hashlib.md5(snippet.encode('utf-8', errors='ignore')).hexdigest()
        return f"{source}:{locator}"
    
    def _blend_documents(self, primary: List[Dict], secondary: List[Dict]) -> List[Dict]:
        if not secondary:
            return primary
        merged = list(primary)
        seen = {self._document_signature(doc) for doc in primary}
        for doc in secondary:
            signature = self._document_signature(doc)
            if signature in seen:
                continue
            merged.append(doc)
            seen.add(signature)
        return merged
    
    def _collect_graph_terms(self, entities: List[Dict], graph_depth: int) -> Tuple[List[str], List[Dict]]:
        terms = []
        graph_context = []
        if not self.graph_db or not self.graph_db.db or not entities:
            return terms, graph_context
        
        for entity in entities[:3]:
            name = entity.get('name')
            if not name:
                continue
            terms.append(name)
            neighbors = self.graph_db.query_neighbors(
                name,
                depth=graph_depth
            )
            if neighbors.get('entities') or neighbors.get('relations'):
                graph_context.append({
                    'center_entity': name,
                    'neighbors': neighbors
                })
            for relation in neighbors.get('relations', [])[:5]:
                rel_type = relation.get('type')
                if rel_type:
                    terms.append(rel_type)
            for neighbor_entity in neighbors.get('entities', [])[:5]:
                neighbor_name = neighbor_entity.get('name') or neighbor_entity.get('display_name') or neighbor_entity.get('normalized_name')
                if neighbor_name and neighbor_name != name:
                    terms.append(neighbor_name)
        
        unique_terms = []
        seen_terms = set()
        for term in terms:
            if term and term not in seen_terms:
                unique_terms.append(term)
                seen_terms.add(term)
            if len(unique_terms) >= 12:
                break
        
        return unique_terms, graph_context
    
    def _graph_expanded_document_search(
        self,
        query: str,
        graph_terms: List[str],
        k_docs: int
    ) -> List[Dict]:
        if not graph_terms:
            return []
        expansion = " ".join(graph_terms)
        expanded_query = f"{query} {expansion}"
        return self.search_documents(expanded_query, k=k_docs, use_reranker=False)
    
    def graph_only_search(
        self,
        query: str,
        k_entities: int = 5,
        graph_depth: int = 1
    ) -> Dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: FAISSë¡œ ì—”í‹°í‹° ì°¾ê³  â†’ ê·¸ë˜í”„ DBë¡œ ê´€ê³„ í™•ì¥
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k_entities: ë°˜í™˜í•  ì—”í‹°í‹° ìˆ˜
            graph_depth: ê·¸ë˜í”„ íƒìƒ‰ ê¹Šì´
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ (ì—”í‹°í‹°, ê´€ê³„, ë¬¸ì„œ ì¶œì²˜)
        """
        print(f"\n[GraphRAG] ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        all_entities = []
        all_relations = []
        all_sources = set()
        graph_context = []
        seen_entities = set()
        
        # 0ë‹¨ê³„: ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        # "Xì— ëŒ€í•´ ì•Œë ¤ì¤˜", "Xê°€ ë­ì•¼", "Xë€?" ë“±ì—ì„œ X ì¶”ì¶œ
        import re
        query_keywords = []
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        patterns = [
            r'^(.+?)(?:ì— ëŒ€í•´|ì— ê´€í•´|ì—ëŒ€í•´|ì—ê´€í•´|ê°€ ë­|ì´ ë­|ë€\?|ì´ë€|ê°€ ë¬´ì—‡|ì´ ë¬´ì—‡|ì€ ë¬´ì—‡|ëŠ” ë¬´ì—‡|ì„ ì•Œë ¤|ë¥¼ ì•Œë ¤|ì— ëŒ€í•œ|ì— ê´€í•œ)',
            r'^(.+?)(?:ì´ë€|ë€|ì´ë¼ëŠ”|ë¼ëŠ”|ì´ë¼ê³ |ë¼ê³ ).*(?:ë­|ë¬´ì—‡|ì•Œë ¤)',
            r'^(.+?)(?:ì„¤ëª…|ì•Œë ¤ì¤˜|ì•Œë ¤ì£¼ì„¸ìš”|ì•Œë ¤ ì¤˜|ì•Œë ¤ ì£¼ì„¸ìš”)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query)
            if match:
                keyword = match.group(1).strip()
                # ë¶ˆí•„ìš”í•œ ì¡°ì‚¬ ì œê±°
                keyword = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì™€|ê³¼|ì—ì„œ|ì—ê²Œ|í•œí…Œ|ë¡œ|ìœ¼ë¡œ)$', '', keyword)
                if keyword and len(keyword) >= 2:
                    query_keywords.append(keyword)
                    print(f"[GraphRAG] ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ: '{keyword}'")
                break
        
        # í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆì„ ë•Œë§Œ ëª…ì‚¬ ë¶„ë¦¬ ì‹œë„
        if not query_keywords:
            # ê°„ë‹¨í•œ ëª…ì‚¬ ì¶”ì¶œ: ì¡°ì‚¬/ì–´ë¯¸ ì œê±° í›„ 3ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ
            words = query.replace('?', '').replace('!', '').replace('.', '').split()
            for word in words:
                # ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
                cleaned = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì™€|ê³¼|ì—ì„œ|ì—ê²Œ|í•œí…Œ|ë¡œ|ìœ¼ë¡œ|ì—|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ë¼ê³ |ì´ë¼ê³ |ë¼ëŠ”|ì´ë¼ëŠ”|ë€|ì´ë€|ì•¼|ì´ì•¼|ìš”|ì´ìš”|ì£ |ì§€ìš”|ë„¤|êµ°|êµ¬ë‚˜)$', '', word)
                # 3ê¸€ì ì´ìƒë§Œ (ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸)
                if cleaned and len(cleaned) >= 3 and cleaned not in query_keywords:
                    query_keywords.append(cleaned)
            if query_keywords:
                print(f"[GraphRAG] ëª…ì‚¬ ì¶”ì¶œ ê²°ê³¼: {query_keywords}")
        
        # 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œë¡œ GraphDB ì§ì ‘ ê²€ìƒ‰
        print(f"[GraphRAG] 1ë‹¨ê³„: ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ GraphDB ì§ì ‘ ê²€ìƒ‰...")
        
        # ë©”ì¸ í‚¤ì›Œë“œ
        main_keyword = None
        main_keyword_url = ''
        
        # GraphDBì—ì„œ í‚¤ì›Œë“œì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì—”í‹°í‹°ë§Œ ê²€ìƒ‰
        if self.graph_db and self.graph_db.db:
            for kw in query_keywords:
                print(f"[GraphRAG] GraphDBì—ì„œ '{kw}' ê²€ìƒ‰ ì¤‘...")
                
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì—”í‹°í‹°ë§Œ ê²€ìƒ‰ (ë¶€ë¶„ ë§¤ì¹­ ì œì™¸)
                try:
                    find_query = """
                    FOR e IN entities
                        FILTER e.name == @name
                        LIMIT 5
                        RETURN e
                    """
                    cursor = self.graph_db.db.aql.execute(find_query, bind_vars={'name': kw})
                    found_entities = list(cursor)
                    
                    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì—”í‹°í‹° ê²€ìƒ‰
                    # (ë‹¨, í‚¤ì›Œë“œ ê¸¸ì´ê°€ 3ì ì´ìƒì¼ ë•Œë§Œ)
                    if not found_entities and len(kw) >= 3:
                        find_query2 = """
                        FOR e IN entities
                            FILTER CONTAINS(e.name, @name)
                            SORT LENGTH(e.name) ASC
                            LIMIT 3
                            RETURN e
                        """
                        cursor = self.graph_db.db.aql.execute(find_query2, bind_vars={'name': kw})
                        found_entities = list(cursor)
                    
                    for ent in found_entities:
                        name = ent.get('name')
                        if name and name not in seen_entities:
                            seen_entities.add(name)
                            entity_data = {
                                'name': name,
                                'type': ent.get('type', 'unknown'),
                                'sources': ent.get('sources', []),
                                'definition': ent.get('definition', ''),
                                'summary': ent.get('summary', ''),
                                'url': ent.get('url', ''),
                                'category': ent.get('category', '')
                            }
                            all_entities.append(entity_data)
                            print(f"[GraphRAG] GraphDBì—ì„œ ë°œê²¬: '{name}' (type: {ent.get('type', 'unknown')})")
                except Exception as e:
                    print(f"[GraphRAG] GraphDB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        if not all_entities:
            print(f"[GraphRAG] GraphDBì—ì„œ ì¼ì¹˜í•˜ëŠ” ì—”í‹°í‹° ì—†ìŒ")
        
        # 2ë‹¨ê³„: ê·¸ë˜í”„ DBì—ì„œ ê´€ê³„ í™•ì¥
        if not self.graph_db or not self.graph_db.db:
            print("[GraphRAG] ê²½ê³ : ê·¸ë˜í”„ DBê°€ ì—°ê²°ë˜ì§€ ì•ŠìŒ!")
            return {
                'query': query,
                'main_keyword': None,
                'main_keyword_url': '',
                'entities': all_entities,
                'relations': [],
                'graph_context': [],
                'sources': []
            }
        
        print(f"[GraphRAG] 2ë‹¨ê³„: ê·¸ë˜í”„ DB ê´€ê³„ í™•ì¥...")
        
        # 1ë‹¨ê³„ì—ì„œ ì°¾ì€ ì—”í‹°í‹°ë“¤ì˜ ê·¸ë˜í”„ ê´€ê³„ ì¡°íšŒ
        # ì²« ë²ˆì§¸ ì—”í‹°í‹°ë¥¼ ë©”ì¸ í‚¤ì›Œë“œë¡œ ì„¤ì •
        for entity in all_entities[:5]:  # ìƒìœ„ 5ê°œ ì—”í‹°í‹°
            name = entity['name']
            print(f"[GraphRAG] '{name}' ê´€ê³„ ì¡°íšŒ ì¤‘...")
            
            # ë©”ì¸ í‚¤ì›Œë“œ ì„¤ì • (ì²« ë²ˆì§¸ ì—”í‹°í‹°)
            if main_keyword is None:
                main_keyword = name
                # URLì€ ì´ë¯¸ 1ë‹¨ê³„ì—ì„œ ê°€ì ¸ì˜¨ ê²½ìš°
                main_keyword_url = entity.get('url', '')
                if not main_keyword_url:
                    # sourcesì—ì„œ ë°±ê³¼ì‚¬ì „ URL ì°¾ê¸°
                    for src in entity.get('sources', []):
                        if isinstance(src, dict):
                            if src.get('doc') == 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „' and src.get('url'):
                                main_keyword_url = src.get('url')
                                break
                print(f"[GraphRAG] ë©”ì¸ í‚¤ì›Œë“œ ì„¤ì •: '{main_keyword}' (URL: {main_keyword_url[:50] if main_keyword_url else 'None'}...)")
            
            neighbors = self.graph_db.query_neighbors(name, depth=graph_depth)
            
            if neighbors.get('entities') or neighbors.get('relations'):
                
                graph_context.append({
                    'center_entity': name,
                    'neighbors': neighbors
                })
                print(f"[GraphRAG] '{name}' â†’ ì´ì›ƒ: {len(neighbors.get('entities', []))}ê°œ, ê´€ê³„: {len(neighbors.get('relations', []))}ê°œ")
            
            # ê´€ê³„ëœ ì—”í‹°í‹° ì¶”ê°€
            for ent in neighbors.get('entities', [])[:k_entities]:
                neighbor_name = ent.get('name')
                if neighbor_name and neighbor_name not in seen_entities:
                    seen_entities.add(neighbor_name)
                    all_entities.append({
                        'name': neighbor_name,
                        'type': ent.get('type', 'unknown'),
                        'sources': ent.get('sources', [])
                    })
                    for src in ent.get('sources', []):
                        if isinstance(src, dict) and src.get('doc'):
                            all_sources.add(src.get('doc'))
            
            # ê´€ê³„ ì¶”ê°€
            for rel in neighbors.get('relations', []):
                triple = rel.get('triple', {})
                if triple:
                    all_relations.append({
                        'subject': triple.get('subject'),
                        'predicate': triple.get('predicate'),
                        'object': triple.get('object'),
                        'source': rel.get('source', {}).get('doc') if isinstance(rel.get('source'), dict) else rel.get('source')
                    })
                    if isinstance(rel.get('source'), dict) and rel.get('source', {}).get('doc'):
                        all_sources.add(rel.get('source', {}).get('doc'))
        
        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        print(f"\n[GraphRAG] ê²€ìƒ‰ ì™„ë£Œ:")
        print(f"  - ë©”ì¸ í‚¤ì›Œë“œ: {main_keyword}")
        print(f"  - ì—”í‹°í‹°: {len(all_entities)}ê°œ")
        print(f"  - ê´€ê³„: {len(all_relations)}ê°œ")
        print(f"  - ì¶œì²˜: {len(all_sources)}ê°œ")
        if all_entities:
            print(f"  - ì—”í‹°í‹° ëª©ë¡: {[e['name'] for e in all_entities[:5]]}")
        if all_relations:
            print(f"  - ê´€ê³„ ì˜ˆì‹œ: {all_relations[0] if all_relations else 'None'}")
        
        return {
            'query': query,
            'main_keyword': main_keyword,  # ë©”ì¸ í‚¤ì›Œë“œ (FAISS ê²°ê³¼ ì¤‘ GraphDBì— ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸)
            'main_keyword_url': main_keyword_url,  # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ URL
            'entities': all_entities[:k_entities],
            'relations': all_relations[:10],
            'graph_context': graph_context,
            'sources': list(all_sources)
        }
    
    def hybrid_search(
        self,
        query: str,
        k_docs: int = 3,
        k_entities: int = 3,
        graph_depth: int = 1
    ) -> Dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + ê·¸ë˜í”„)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k_docs: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            k_entities: ë°˜í™˜í•  ì—”í‹°í‹° ìˆ˜
            graph_depth: ê·¸ë˜í”„ íƒìƒ‰ ê¹Šì´
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ (ë¬¸ì„œ, ì—”í‹°í‹°, ê·¸ë˜í”„ ì •ë³´)
        """
        print(f"\ní•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: '{query}'")
        
        # 1. ë¬¸ì„œ ë²¡í„° ê²€ìƒ‰
        documents = self.search_documents(query, k=k_docs)
        
        # 2. ì—”í‹°í‹° KNN ê²€ìƒ‰
        entities = self.search_entities_knn(query, k=k_entities)
        
        # 3. ê·¸ë˜í”„ í‚¤ì›Œë“œ/ë¬¸ì„œ í™•ì¥
        graph_documents = []
        graph_terms, graph_context = self._collect_graph_terms(entities, graph_depth)
        if graph_terms:
            graph_documents = self._graph_expanded_document_search(query, graph_terms, k_docs)
            documents = self._blend_documents(documents, graph_documents)
        
        return {
            'query': query,
            'documents': documents,
            'entities': entities,
            'graph_context': graph_context,
            'graph_terms': graph_terms,
            'graph_documents': graph_documents
        }
    
    def generate_answer(
        self,
        query: str,
        use_graph: bool = True
    ) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ìš°ì„ )
        
        Args:
            query: ì§ˆë¬¸
            use_graph: ê·¸ë˜í”„ ì •ë³´ ì‚¬ìš© ì—¬ë¶€ (í•­ìƒ Trueë¡œ ë™ì‘)
            
        Returns:
            ë‹µë³€ í…ìŠ¤íŠ¸
            
        ìš°ì„ ìˆœìœ„:
            1. í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì •ë³´ (definition, summary)
            2. ê´€ë ¨ ì‚¬ë£Œ ì •ë³´ (ì„ ì¡°ì‹¤ë¡, ë‚œì¤‘ì¼ê¸° ë“±)
            3. ì •ë³´ ë¶€ì¡± ì‹œ FAISS ë²¡í„° ê²€ìƒ‰ ë³´ì™„
        """
        if not self.llm:
            return "LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ê·¸ë˜í”„ DB ê¸°ë°˜ ê²€ìƒ‰
        results = self.graph_only_search(query, k_entities=10, graph_depth=1)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ìš°ì„ ìˆœìœ„ë³„ë¡œ ë¶„ë¦¬)
        encyclopedia_parts = []  # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „
        historical_parts = []    # ì—­ì‚¬ ì‚¬ë£Œ (ì„ ì¡°ì‹¤ë¡, ë‚œì¤‘ì¼ê¸° ë“±)
        relation_parts = []      # ì§€ì‹ ê·¸ë˜í”„ ê´€ê³„
        
        # 1ìˆœìœ„: í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì •ë³´ ì¶”ì¶œ
        for entity in results.get('entities', []):
            entity_name = entity.get('name', '')
            entity_type = entity.get('type', '')
            definition = entity.get('definition', '')
            summary = entity.get('summary', '')
            sources = entity.get('sources', [])
            
            # ë°±ê³¼ì‚¬ì „ ì •ë³´ í™•ì¸
            has_encyclopedia = False
            encyclopedia_snippet = ''
            historical_snippets = []
            
            for src in sources if isinstance(sources, list) else []:
                if not isinstance(src, dict):
                    continue
                src_type = src.get('type', '')
                src_doc = src.get('doc', '')
                snippet = src.get('snippet', '')
                
                if 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „' in src_type:
                    has_encyclopedia = True
                    encyclopedia_snippet = snippet or definition or summary
                else:
                    # ì¼ë°˜ ì‚¬ë£Œ
                    if snippet:
                        # ì¶œì²˜ëª… ì •ë¦¬
                        doc_name = src_doc
                        for ext in ['.pdf', '.json', '.txt']:
                            doc_name = doc_name.replace(ext, '')
                        if '_' in doc_name:
                            doc_name = doc_name.replace('_', ' ')
                        historical_snippets.append((doc_name, snippet))
            
            # ë°±ê³¼ì‚¬ì „ ì •ë³´ ì¶”ê°€ (1ìˆœìœ„)
            if has_encyclopedia and encyclopedia_snippet:
                info = f"### {entity_name} ({entity_type})\n"
                info += f"{encyclopedia_snippet[:500]}"
                if definition and definition not in encyclopedia_snippet:
                    info += f"\nì •ì˜: {definition[:300]}"
                encyclopedia_parts.append(info)
        
            # ì‚¬ë£Œ ì •ë³´ ì¶”ê°€ (2ìˆœìœ„)
            for doc_name, snippet in historical_snippets[:3]:
                info = f"- [{doc_name}] {entity_name}: {snippet[:200]}"
                historical_parts.append(info)
        
        # ì§€ì‹ ê·¸ë˜í”„ ê´€ê³„ (3ìˆœìœ„)
        for rel in results.get('relations', [])[:10]:
            subject = rel.get('subject', '')
            predicate = rel.get('predicate', '')
            obj = rel.get('object', '')
            if subject and predicate and obj:
                relation_parts.append(f"- {subject} --[{predicate}]--> {obj}")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
        context_parts = []
        
        if encyclopedia_parts:
            context_parts.append("## í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì •ë³´ (ì‹ ë¢°ë„ ë†’ìŒ):")
            context_parts.extend(encyclopedia_parts[:5])
        
        if historical_parts:
            context_parts.append("\n\n## ê´€ë ¨ ì—­ì‚¬ ì‚¬ë£Œ:")
            context_parts.extend(historical_parts[:10])
        
        if relation_parts:
            context_parts.append("\n\n## ì§€ì‹ ê·¸ë˜í”„ ê´€ê³„:")
            context_parts.extend(relation_parts[:10])
        
        # ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ FAISS ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
        faiss_sources = []  # FAISSì—ì„œ ì°¾ì€ ì¶œì²˜ ì €ì¥
        if len(encyclopedia_parts) < 2 and len(historical_parts) < 3:
            if self.vectorstore:
                print("[ë‹µë³€ ìƒì„±] ì •ë³´ ë¶€ì¡± - FAISS ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„")
                try:
                    faiss_docs = self.vectorstore.similarity_search(query, k=3)
                    if faiss_docs:
                        context_parts.append("\n\n## ì¶”ê°€ ì°¸ê³  ë¬¸ì„œ (ë²¡í„° ê²€ìƒ‰):")
                        for doc in faiss_docs:
                            content = doc.page_content[:300]
                            source = doc.metadata.get('source', '')
                            if source:
                                source_name = source.split('/')[-1].split('\\')[-1]
                                for ext in ['.pdf', '.json', '.txt']:
                                    source_name = source_name.replace(ext, '')
                                context_parts.append(f"- [{source_name}] {content}")
                                # FAISS ì¶œì²˜ ì €ì¥ (ë‚˜ì¤‘ì— ì°¸ê³  ë¬¸ì„œì— ì¶”ê°€)
                                if source_name:
                                    faiss_sources.append(source_name)
                            else:
                                context_parts.append(f"- {content}")
                except Exception as e:
                    print(f"FAISS ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        context = "\n".join(context_parts)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ì—­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

**ì¤‘ìš”**: í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ì •ë³´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ê³ , 
ì—­ì‚¬ ì‚¬ë£Œì˜ ë‚´ìš©ì„ ë³´ì¶© ì„¤ëª…ì— í™œìš©í•˜ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€ ì‘ì„± ê°€ì´ë“œ:
1. í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ì˜ ì •ì˜ì™€ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ì‘ì„±
2. ì—­ì‚¬ ì‚¬ë£Œì˜ êµ¬ì²´ì ì¸ ê¸°ë¡ì„ ì¸ìš©í•˜ì—¬ ë³´ì¶©
3. ì—­ì‚¬ì  ì‚¬ì‹¤ì„ ì •í™•í•˜ê²Œ ê¸°ìˆ 
4. 5-7ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ì„¤ëª…

ë‹µë³€:"""
        
        print(f"\n{self.llm.model} LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
        
        try:
            response = self.llm.invoke(prompt)
            
            # ë‹µë³€ì—ì„œ ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±° (## ì œëª© ë“±)
            import re
            cleaned_response = response.strip()
            # ì²« ì¤„ì´ ë§ˆí¬ë‹¤ìš´ í—¤ë”ë©´ ì œê±°
            cleaned_response = re.sub(r'^#{1,6}\s+.+?\n+', '', cleaned_response)
            # ì¤‘ê°„ì— ìˆëŠ” ë§ˆí¬ë‹¤ìš´ í—¤ë”ë„ ì œê±°
            cleaned_response = re.sub(r'\n#{1,6}\s+.+?\n', '\n', cleaned_response)
            
            # ë‹µë³€ êµ¬ì„±
            answer_parts = [cleaned_response.strip()]
            
            # ë©”ì¸ í‚¤ì›Œë“œ (FAISS ê²€ìƒ‰ ê²°ê³¼ ì¤‘ GraphDBì— ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸)
            main_keyword = results.get('main_keyword', '')
            main_keyword_url = results.get('main_keyword_url', '')
            
            # ê´€ë ¨ ì¶œì²˜ ìˆ˜ì§‘ (ì—”í‹°í‹°ì˜ sourcesì—ì„œ)
            doc_sources = []  # [(ì¶œì²˜ëª…, url), ...]
            seen_sources = set()
            
            # 1. í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ (ë©”ì¸ í‚¤ì›Œë“œ URLë¡œ í•˜ì´í¼ë§í¬)
            if main_keyword_url:
                doc_sources.append(('í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „', main_keyword_url))
                seen_sources.add('í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „')
            
            # 2. ë‹¤ë¥¸ ì‚¬ë£Œë“¤ ìˆ˜ì§‘ (ë‚œì¤‘ì¼ê¸°, ì„ ì¡°ì‹¤ë¡ ë“±)
            for entity in results.get('entities', []):
                sources = entity.get('sources', [])
                for src in sources if isinstance(sources, list) else []:
                    if not isinstance(src, dict):
                        continue
                    
                    src_doc = src.get('doc', '')
                    src_type = src.get('type', '')
                    
                    # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ì€ ì´ë¯¸ ì¶”ê°€ë¨
                    if src_doc == 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „' or 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „' in str(src_type):
                        continue
                    
                    # ì¶œì²˜ëª… ì •ë¦¬
                    source_name = src_doc or src_type or ''
                    for ext in ['.pdf', '.json', '.txt', '.md', '.docx']:
                        source_name = source_name.replace(ext, '')
                    if '_' in source_name:
                        source_name = source_name.replace('_', ' ')
                    
                    if source_name and source_name not in seen_sources:
                        seen_sources.add(source_name)
                        doc_sources.append((source_name, ''))  # ë‹¤ë¥¸ ì‚¬ë£ŒëŠ” URL ì—†ìŒ
            
            # 3. results.sourcesì—ì„œë„ ì¶”ê°€ (fallback)
            for source in results.get('sources', []):
                if source:
                    source_name = source.split('/')[-1].split('\\')[-1]
                    for ext in ['.pdf', '.json', '.txt', '.md', '.docx']:
                        source_name = source_name.replace(ext, '')
                    if '_' in source_name:
                        source_name = source_name.replace('_', ' ')
                    if source_name and source_name not in seen_sources:
                        seen_sources.add(source_name)
                        doc_sources.append((source_name, ''))
            
            # 4. FAISS ê²€ìƒ‰ì—ì„œ ì°¾ì€ ì¶œì²˜ ì¶”ê°€
            for faiss_source in faiss_sources:
                # ì¶œì²˜ëª… ì •ë¦¬
                source_name = faiss_source
                for ext in ['.pdf', '.json', '.txt', '.md', '.docx']:
                    source_name = source_name.replace(ext, '')
                if '_' in source_name:
                    source_name = source_name.replace('_', ' ')  # _ â†’ ë„ì–´ì“°ê¸°
                if source_name and source_name not in seen_sources:
                    seen_sources.add(source_name)
                    doc_sources.append((source_name, ''))
            
            # ì°¸ê³  ë¬¸ì„œ í¬ë§·íŒ…
            if doc_sources:
                ref_parts = []
                for name, url in doc_sources[:10]:  # ìµœëŒ€ 10ê°œ
                    if url:
                        ref_parts.append(f"[{name}]({url})")
                    else:
                        ref_parts.append(name)
                answer_parts.append("\n\nì°¸ê³  ë¬¸ì„œ: " + ", ".join(ref_parts))
            
            return "\n".join(answer_parts)
            
        except Exception as e:
            print(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def save_indexes(self, save_dir: str = 'graphrag_index'):
        """ì¸ë±ìŠ¤ ì €ì¥
        
        Args:
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        os.makedirs(save_dir, exist_ok=True)
        saved_count = 0
        
        # ë¬¸ì„œ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        if self.vectorstore:
            doc_path = os.path.join(save_dir, 'documents')
            self.vectorstore.save_local(doc_path)
            print(f"  âœ“ ë¬¸ì„œ ì¸ë±ìŠ¤: {doc_path}/")
            saved_count += 1
        
        # ì—”í‹°í‹° ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        if self.entity_vectorstore:
            entity_path = os.path.join(save_dir, 'entities')
            self.entity_vectorstore.save_local(entity_path)
            print(f"  âœ“ ì—”í‹°í‹° ì¸ë±ìŠ¤: {entity_path}/")
            saved_count += 1
        
        # ì§€ì‹ ê·¸ë˜í”„ ì €ì¥ (JSON ë°±ì—…)
        if self.graph_db and self.graph_db.db:
            graph_path = os.path.join(save_dir, 'knowledge_graph.json')
            success = self.graph_db.export_graph(graph_path)
            if success:
                print(f"  âœ“ ì§€ì‹ ê·¸ë˜í”„: {graph_path}")
                saved_count += 1
        
        print(f"\nì´ {saved_count}ê°œ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
    
    def load_indexes(self, load_dir: str = 'graphrag_index'):
        """ì¸ë±ìŠ¤ ë¡œë“œ (ë²¡í„° + ì§€ì‹ ê·¸ë˜í”„)
        
        Args:
            load_dir: ë¡œë“œ ë””ë ‰í† ë¦¬
        """
        loaded_count = 0
        
        # ë¬¸ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        doc_index_file = os.path.join(load_dir, 'documents', 'index.faiss')
        if os.path.exists(doc_index_file):
            doc_path = os.path.join(load_dir, 'documents')
            self.vectorstore = FAISS.load_local(
                doc_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print(f"  âœ“ ë¬¸ì„œ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            loaded_count += 1
        else:
            print(f"  âœ— ë¬¸ì„œ ì¸ë±ìŠ¤ ì—†ìŒ")
        
        # ì—”í‹°í‹° ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        entity_index_file = os.path.join(load_dir, 'entities', 'index.faiss')
        if os.path.exists(entity_index_file):
            entity_path = os.path.join(load_dir, 'entities')
            self.entity_vectorstore = FAISS.load_local(
                entity_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print(f"  âœ“ ì—”í‹°í‹° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            loaded_count += 1
        else:
            print(f"  âœ— ì—”í‹°í‹° ì¸ë±ìŠ¤ ì—†ìŒ")
        
        # ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ (JSON â†’ ArangoDB)
        graph_path = os.path.join(load_dir, 'knowledge_graph.json')
        if os.path.exists(graph_path) and self.graph_db and self.graph_db.db:
            # ArangoDBì— ë°ì´í„° ë¡œë“œ
            success = self.graph_db.import_graph(graph_path)
            if success:
                print(f"  âœ“ ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ (ArangoDB)")
                loaded_count += 1
            else:
                print(f"  âœ— ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨")
        else:
            if not os.path.exists(graph_path):
                print(f"  âœ— ì§€ì‹ ê·¸ë˜í”„ íŒŒì¼ ì—†ìŒ")
            elif not self.graph_db or not self.graph_db.db:
                print(f"  âœ— ArangoDB ì—°ê²° ì—†ìŒ (ì§€ì‹ ê·¸ë˜í”„ ê±´ë„ˆëœ€)")
        
        if loaded_count == 0:
            raise Exception("ë¡œë“œí•  ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")


def _copy_encykorea_to_global(encykorea_db: str, global_db: str):
    """í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ DBë¥¼ í†µí•© ê·¸ë˜í”„ DBë¡œ ë³µì‚¬
    
    Args:
        encykorea_db: ë°±ê³¼ì‚¬ì „ DB ì´ë¦„ (kg_encykorea)
        global_db: í†µí•© ê·¸ë˜í”„ DB ì´ë¦„ (knowledge_graph)
    """
    from arango import ArangoClient
    
    client = ArangoClient(hosts='http://localhost:8530')
    sys_db = client.db('_system', username='root', password='')
    
    # ë°±ê³¼ì‚¬ì „ DB ì¡´ì¬ í™•ì¸
    if not sys_db.has_database(encykorea_db):
        raise Exception(f"ë°±ê³¼ì‚¬ì „ DB '{encykorea_db}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # í†µí•© DB ì´ˆê¸°í™” (ì‚­ì œ í›„ ì¬ìƒì„±)
    if sys_db.has_database(global_db):
        sys_db.delete_database(global_db)
        print(f"  - ê¸°ì¡´ í†µí•© DB '{global_db}' ì‚­ì œ")
    
    sys_db.create_database(global_db)
    print(f"  - í†µí•© DB '{global_db}' ìƒì„±")
    
    # ë°±ê³¼ì‚¬ì „ DB ì—°ê²°
    ency_db = client.db(encykorea_db, username='root', password='')
    global_db_conn = client.db(global_db, username='root', password='')
    
    # ì»¬ë ‰ì…˜ ë³µì‚¬
    for col_name in ['entities', 'relations']:
        if ency_db.has_collection(col_name):
            # í†µí•© DBì— ì»¬ë ‰ì…˜ ìƒì„±
            is_edge = (col_name == 'relations')
            if is_edge:
                global_db_conn.create_collection(col_name, edge=True)
            else:
                global_db_conn.create_collection(col_name)
            
            # ë°ì´í„° ë³µì‚¬
            src_col = ency_db.collection(col_name)
            dst_col = global_db_conn.collection(col_name)
            
            # ë°°ì¹˜ë¡œ ë³µì‚¬ (ì„±ëŠ¥ ìµœì í™”)
            batch_size = 1000
            cursor = src_col.all()
            batch = []
            total_copied = 0
            
            for doc in cursor:
                # _id, _rev ì œê±° (ìƒˆ DBì—ì„œ ìë™ ìƒì„±)
                doc_copy = {k: v for k, v in doc.items() if k not in ['_id', '_rev']}
                batch.append(doc_copy)
                
                if len(batch) >= batch_size:
                    try:
                        dst_col.insert_many(batch)
                        total_copied += len(batch)
                    except Exception as e:
                        # ê°œë³„ ì‚½ì… ì‹œë„
                        for d in batch:
                            try:
                                dst_col.insert(d)
                                total_copied += 1
                            except:
                                pass
                    batch = []
            
            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if batch:
                try:
                    dst_col.insert_many(batch)
                    total_copied += len(batch)
                except Exception as e:
                    for d in batch:
                        try:
                            dst_col.insert(d)
                            total_copied += 1
                        except:
                            pass
            
            print(f"  - {col_name}: {total_copied}ê°œ ë³µì‚¬")


def main():
    """GraphRAG ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print("GraphRAG ì‹œìŠ¤í…œ - ì§€ì‹ ê·¸ë˜í”„ + ë²¡í„° ê²€ìƒ‰")
    BASE_DATA_DIR = 'graphrag_data'
    OUTPUT_DIR = 'output'
    
    print(f"\nì…ë ¥ ë¬¸ì„œ ë£¨íŠ¸: {OUTPUT_DIR}/")
    print(f"ì¸ë±ìŠ¤ ì €ì¥ ë£¨íŠ¸: {BASE_DATA_DIR}/")
    
    if not os.path.exists(OUTPUT_DIR):
        print(f"ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {OUTPUT_DIR}")
        return
    
    def slugify(name: str) -> str:
        if not name:
            return 'source'
        ascii_only = re.sub(r'[^a-zA-Z0-9_-]', '_', name)
        ascii_only = re.sub(r'_+', '_', ascii_only).strip('_').lower()
        return ascii_only or 'source'
    
    # ì‚¬ë£Œëª… â†’ ì˜ë¬¸ ìŠ¬ëŸ¬ê·¸ ë§¤í•‘ (í•„ìš” ì‹œ í™•ì¥)
    custom_slugs = {
        'ì§•ë¹„ë¡': 'jingbirok',
        'ì¡°ì„ ì™•ì¡°ì‹¤ë¡': 'joseon',
        'ì¬ì¡°ë²ˆë°©ì§€': 'jaejo',
        'ì—°ë ¤ì‹¤ê¸°ìˆ ': 'yeollyeo',
        'ë‚œì¤‘ì¡ë¡': 'japrok',
        'ê¸°ì¬ì‚¬ì´ˆ': 'gijae',
        'ê³ ëŒ€ì¼ë¡': 'godae',
        'ê°„ì–‘ë¡': 'ganyang',
        'ë‚œì¤‘ì¼ê¸°': 'najung',
        'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „': 'encykorea'
    }
    # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ì€ ì´ë¯¸ êµ¬ì¶•ë¨ (kg_encykorea) - ì´ˆê¸°í™” ê¸ˆì§€
    # í†µí•© ê·¸ë˜í”„(knowledge_graph)ëŠ” kg_encykoreaë¥¼ ë¨¼ì € ë³µì‚¬í•œ í›„ ë‹¤ë¥¸ ì†ŒìŠ¤ ì¶”ê°€
    ENCYKOREA_DB = 'kg_encykorea'  # ë°±ê³¼ì‚¬ì „ DB (ì ˆëŒ€ ì´ˆê¸°í™” ì•ˆ ë¨)
    
    TARGET_SOURCES = [
        # 'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „',  # ì´ë¯¸ êµ¬ì¶•ë¨ - ì œì™¸
        'ì—°ë ¤ì‹¤ê¸°ìˆ ',
        'ê³ ëŒ€ì¼ë¡',
        # 'ë‚œì¤‘ì¡ë¡'
    ]
    # ============================================================

    # í•˜ìœ„ ì‚¬ë£Œ ë””ë ‰í† ë¦¬ íƒìƒ‰ (TARGET_SOURCESì— ì§€ì •ëœ ê²ƒë§Œ)
    candidate_dirs = []
    for source_name in TARGET_SOURCES:
        path = os.path.join(OUTPUT_DIR, source_name)
        if os.path.isdir(path):
            candidate_dirs.append(path)
            print(f"  âœ“ ì²˜ë¦¬ ëŒ€ìƒ: {source_name}")
        else:
            print(f"  âš  í´ë” ì—†ìŒ: {source_name} ({path})")
    
    if not candidate_dirs:
        print(f"\nâš ï¸  ì²˜ë¦¬í•  ì‚¬ë£Œê°€ ì—†ìŠµë‹ˆë‹¤. TARGET_SOURCESë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   í˜„ì¬ ì„¤ì •: {TARGET_SOURCES}")
        return
    
    global_db_name = os.getenv('GLOBAL_GRAPH_DB', 'knowledge_graph')
    env_global_reset = os.getenv('GLOBAL_GRAPH_RESET')
    if env_global_reset is None:
        global_reset_remaining = True  # ì²« ì‹¤í–‰ì€ í•­ìƒ ì´ˆê¸°í™”
    else:
        global_reset_remaining = env_global_reset.lower() in ('1', 'true', 'yes')
    
    # í†µí•© ê·¸ë˜í”„ ì´ˆê¸°í™” ì‹œ í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ë°ì´í„° ë¨¼ì € ë³µì‚¬
    if global_reset_remaining:
        print(f"\nğŸ“š í†µí•© ê·¸ë˜í”„ ì´ˆê¸°í™”: {ENCYKOREA_DB} â†’ {global_db_name}")
        try:
            from graph_db import ArangoGraphDB
            # ë°±ê³¼ì‚¬ì „ DBì—ì„œ í†µí•© DBë¡œ ë³µì‚¬
            _copy_encykorea_to_global(ENCYKOREA_DB, global_db_name)
            global_reset_remaining = False  # ë³µì‚¬ ì™„ë£Œ í›„ ì´ˆê¸°í™” ë¹„í™œì„±í™”
            print(f"  âœ“ ë°±ê³¼ì‚¬ì „ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš  ë°±ê³¼ì‚¬ì „ ë°ì´í„° ë³µì‚¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def build_source_db_name(slug: str) -> str:
        env_db = os.getenv('SOURCE_GRAPH_DB')
        if env_db:
            return env_db
        prefix = os.getenv('SOURCE_GRAPH_PREFIX', 'kg')
        return f"{prefix}_{slug}"
    
    last_graphrag = None
    processed_sources = 0
    
    for collection_path in candidate_dirs:
        collection_name = os.path.basename(collection_path.rstrip(os.sep))
        collection_slug = custom_slugs.get(collection_name)
        if not collection_slug:
            slug_candidate = slugify(collection_name)
            if slug_candidate == 'source':
                slug_candidate = f"source_{abs(hash(collection_name)) & 0xffffffff:08x}"
            collection_slug = slug_candidate
        source_db_name = build_source_db_name(collection_slug)
        data_dir = os.path.join(BASE_DATA_DIR, collection_slug)
        os.makedirs(data_dir, exist_ok=True)
        
        print("\n" + "=" * 80)
        print(f"ì‚¬ë£Œ ì²˜ë¦¬: {collection_name}")
        print(f"  - ì›ë³¸ ê²½ë¡œ: {collection_path}")
        print(f"  - ê°œë³„ DB: {source_db_name}")
        print(f"  - ì¸ë±ìŠ¤ ê²½ë¡œ: {data_dir}/")
        print(f"  - í†µí•© DB: {global_db_name} (reset={global_reset_remaining})")
        
        documents = load_documents_from_output(collection_path)
        if not documents:
            print("  âš ï¸  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        graphrag = GraphRAGSystem(
            embedding_model_name='intfloat/multilingual-e5-large-instruct',
            llm_model_name='gemma3:12b',
            arango_host='localhost',
            arango_port=8530,  # ìƒˆ ArangoDB í¬íŠ¸ (ê¸°ì¡´ 8529ëŠ” ìœ ì§€)
            arango_password='',
            arango_db_name=source_db_name,
            arango_reset=True,
            global_arango_db_name=global_db_name,
            global_arango_reset=global_reset_remaining,
            use_reranker=True,
            use_tika=False
        )
        global_reset_remaining = False
        last_graphrag = graphrag
        processed_sources += 1
        
        doc_index_path = os.path.join(data_dir, 'documents', 'index.faiss')
        entity_index_path = os.path.join(data_dir, 'entities', 'index.faiss')
        graph_json_path = os.path.join(data_dir, 'knowledge_graph.json')
        
        has_doc_index = os.path.exists(doc_index_path)
        has_entity_index = os.path.exists(entity_index_path)
        has_graph_json = os.path.exists(graph_json_path)
        
        # ê°•ì œ ì¬êµ¬ì¶• ì—¬ë¶€ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ëª…ë ¹ì¤„ ì¸ìë¡œ ì„¤ì • ê°€ëŠ¥)
        force_rebuild = os.getenv('FORCE_REBUILD', 'false').lower() in ('1', 'true', 'yes')
        
        if not force_rebuild and (has_doc_index or has_entity_index or has_graph_json):
            print(f"\nâœ“ ê¸°ì¡´ ì¸ë±ìŠ¤ ë°œê²¬: {data_dir}")
            if has_doc_index:
                print("  - ë¬¸ì„œ ì¸ë±ìŠ¤: âœ“")
            if has_entity_index:
                print("  - ì—”í‹°í‹° ì¸ë±ìŠ¤: âœ“")
            if has_graph_json:
                print("  - ì§€ì‹ ê·¸ë˜í”„ ë°±ì—…: âœ“")
            
            print("\nê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„...")
            try:
                graphrag.load_indexes(data_dir)
                print("âœ“ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ (ì¬êµ¬ì¶• ìƒëµ)")
                sample_docs = []
            except Exception as e:
                print(f"âœ— ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤...")
                sample_docs = documents  # ì „ì²´ ë¬¸ì„œ
        else:
            if force_rebuild:
                print(f"\nâš  ê°•ì œ ì¬êµ¬ì¶• ëª¨ë“œ (FORCE_REBUILD=true)")
            print("\nê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.")
            sample_docs = documents  # ì „ì²´ ë¬¸ì„œ
        
        if sample_docs:
            print("\nì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
            print(f"ğŸ“„ ì´ {len(sample_docs)}ê°œ ë¬¸ì„œ ì²˜ë¦¬")
            
            # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ì€ êµ¬ì¡°í™”ëœ ë°ì´í„°ì´ë¯€ë¡œ ë²¡í„° ì¸ë±ìŠ¤ ë¶ˆí•„ìš”
            is_encyclopedia = (
                'í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „' in collection_name or
                'encykorea' in collection_name.lower()
            )
            
            graphrag.build_index(
                documents=sample_docs,
                extract_graph=True,
                skip_vector_index=is_encyclopedia
            )
            
            print("\nì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
            graphrag.save_indexes(data_dir)
            print(f"ì €ì¥ ì™„ë£Œ: {data_dir}/")
    
    if processed_sources == 0:
        print("\nì²˜ë¦¬ëœ ì‚¬ë£Œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\nëª¨ë“  ì‚¬ë£Œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    if not last_graphrag or not last_graphrag.llm:
        print("\nLLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ì–´ ì§ˆì˜ì‘ë‹µì„ ìƒëµí•©ë‹ˆë‹¤.")
        print("Ollama ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print("\në§ˆì§€ë§‰ ì²˜ë¦¬ ì‚¬ë£Œ ê¸°ì¤€ ì˜ˆì‹œ ì§ˆì˜ì‘ë‹µ")
    questions = [
        "í•œì‚°ë„ ëŒ€ì²©ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ì„ì§„ì™œë€ì€ ì–¸ì œ ì‹œì‘ë˜ì—ˆë‚˜ìš”?",
        "ì´ìˆœì‹  ì¥êµ°ì˜ ì£¼ìš” ì—…ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    for idx, question in enumerate(questions, 1):
        print(f"\n[ì§ˆë¬¸ {idx}/{len(questions)}]")
        print("-" * 70)
        
        answer = last_graphrag.generate_answer(question, use_graph=True)
        print(answer)

if __name__ == "__main__":
    main()

