import requests
import json
import re
import csv
import os
from typing import Optional, Dict, List, Any
from pathlib import Path
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ì—‘ì…€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹œë„
try:
    import openpyxl
except ImportError:
    openpyxl = None

def clean_footnote(footnote_text: str) -> List[Dict[str, str]]:
    """
    ë³¸ë¬¸ ì£¼ì„ ì„¤ëª…ì„ LLMì´ í•´ì„í•˜ê¸° ì¢‹ì€ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        list: [{"id": "1", "term": "í›ˆì²™", "definition": "ì„¤ëª…..."}, ...]
    """
    if not footnote_text:
        return []
    
    # \r\nì„ \nìœ¼ë¡œ í†µì¼
    text = footnote_text.replace('\r\n', '\n')
    
    # [^1]: íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
    pattern = r'\[\^(\d+)\]:\s*'
    parts = re.split(pattern, text)
    
    annotations = []
    # partsëŠ” ['', '1', 'ë‚´ìš©1', '2', 'ë‚´ìš©2', ...] í˜•íƒœ
    i = 1
    while i < len(parts) - 1:
        note_id = parts[i]
        content = parts[i + 1].strip()
        
        if content:
            # [â†’ìš°ë¦¬ë§ìƒ˜](URL) íŒ¨í„´ ì œê±°
            content = re.sub(r'\s*\[â†’[^\]]+\]\([^)]+\)', '', content)
            
            # ìš©ì–´ì™€ ì„¤ëª… ë¶„ë¦¬ (ì²« ë²ˆì§¸ ë¬¸ì¥ì´ë‚˜ ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„)
            # ì˜ˆ: "í›ˆì²™(å‹³æˆš) : ëŒ€ëŒ€ë¡œ ë‚˜ë¼ë‚˜..." ë˜ëŠ” "í›ˆì²™. ëŒ€ëŒ€ë¡œ..."
            term = ""
            definition = content
            
            # ì½œë¡ ì´ë‚˜ ë§ˆì¹¨í‘œë¡œ ë¶„ë¦¬ ì‹œë„
            if ': ' in content[:50]:
                parts_split = content.split(': ', 1)
                term = parts_split[0].strip()
                definition = parts_split[1].strip() if len(parts_split) > 1 else content
            elif '. ' in content[:50]:
                parts_split = content.split('. ', 1)
                term = parts_split[0].strip()
                definition = parts_split[1].strip() if len(parts_split) > 1 else content
            
            annotations.append({
                "id": note_id,
                "term": term,
                "definition": definition
            })
        
        i += 2
    
    return annotations


def html_table_to_structured(html_content: str) -> Dict[str, Any]:
    """
    HTML ì½˜í…ì¸ ì—ì„œ í…Œì´ë¸”ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì¶”ì¶œí•˜ê³ ,
    ë‚˜ë¨¸ì§€ ë³¸ë¬¸ì€ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not html_content:
        return {"text": "", "tables": []}
    
    # \r\nì„ ì‹¤ì œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
    text = html_content.replace('\\r\\n', '\n').replace('\r\n', '\n')
    
    # <table> íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
    if '<table' not in text.lower():
        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(text, 'html.parser')
        clean_text = soup.get_text(separator='\n')
        clean_text = re.sub(r'\n{3,}', '\n\n', clean_text)
        return {"text": clean_text.strip(), "tables": []}
    
    soup = BeautifulSoup(text, 'html.parser')
    tables_data = []
    
    # í…Œì´ë¸” ì²˜ë¦¬
    for table in soup.find_all('table'):
        table_info = {"title": "", "headers": [], "rows": []}
        
        # tfootì—ì„œ í…Œì´ë¸” ì œëª© ì¶”ì¶œ
        tfoot = table.find('tfoot')
        if tfoot:
            table_info["title"] = tfoot.get_text(strip=True)
        
        # theadì—ì„œ í—¤ë” ì¶”ì¶œ
        headers = []
        thead = table.find('thead')
        if thead:
            # ë§ˆì§€ë§‰ í—¤ë” í–‰ì—ì„œ ì—´ ì´ë¦„ ì¶”ì¶œ
            header_rows = thead.find_all('tr')
            if header_rows:
                last_header = header_rows[-1]
                cells = last_header.find_all(['th', 'td'])
                headers = [cell.get_text(strip=True) for cell in cells]
        
        # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í–‰ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not headers:
            first_row = table.find('tr')
            if first_row:
                cells = first_row.find_all(['th', 'td'])
                headers = [cell.get_text(strip=True) for cell in cells]
        
        table_info["headers"] = headers
        
        # tbodyì—ì„œ ë°ì´í„° í–‰ ì¶”ì¶œ
        tbody = table.find('tbody')
        rows_to_process = tbody.find_all('tr') if tbody else table.find_all('tr')[1:]
        
        for row in rows_to_process:
            cells = row.find_all(['th', 'td'])
            cell_values = [cell.get_text(strip=True) for cell in cells]
            
            # í—¤ë”ì™€ ë§¤í•‘í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            if headers and len(cell_values) == len(headers):
                row_dict = dict(zip(headers, cell_values))
            else:
                # í—¤ë” ìˆ˜ì™€ ë§ì§€ ì•Šìœ¼ë©´ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
                row_dict = {f"col_{i}": v for i, v in enumerate(cell_values)}
            
            table_info["rows"].append(row_dict)
        
        tables_data.append(table_info)
        
        # í…Œì´ë¸” ìë¦¬ì— í‘œì‹œì ë‚¨ê¸°ê¸°
        table.replace_with(f'[í‘œ: {table_info["title"]}]')
    
    # ë‚˜ë¨¸ì§€ HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    result_text = soup.get_text(separator='\n')
    result_text = re.sub(r'\n{3,}', '\n\n', result_text)
    
    return {"text": result_text.strip(), "tables": tables_data}


def convert_related_articles(related_articles: List[Dict]) -> List[Dict]:
    """
    ê´€ë ¨í•­ëª©ì˜ í•„ë“œëª…ì„ í•œê¸€ë¡œ ë³€í™˜í•˜ê³  writerInfo ì œê±°
    """
    if not related_articles:
        return []
    
    converted = []
    for article in related_articles:
        converted_article = {
            'URL': article.get('url', ''),
            'í•­ëª©ëª…': article.get('headword', ''),
            'ì›ì–´': article.get('origin', ''),
            'í•­ëª© ë¶„ì•¼': article.get('field', ''),
            'í•­ëª© ìœ í˜•': article.get('contentsType', ''),
            'ì‹œëŒ€': article.get('era', ''),
            'í•­ëª© ì •ì˜': article.get('definition', '')
            # writerInfo ì œê±°
        }
        converted.append(converted_article)
    
    return converted

class HistoryAPIClient:
    """í•œêµ­ì—­ì‚¬ì •ë³´í†µí•©ì‹œìŠ¤í…œ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str):
        self.base_url = "https://devin.aks.ac.kr:8080/v1"
        self.headers = {
            "X-API-Key": api_key
        }
    
    def get_all_articles(self, page_no: int = 1) -> Dict[str, Any]:
        url = f"{self.base_url}/Articles"
        params = {"pageNo": page_no}
        response = requests.get(url, headers=self.headers, params=params)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def search_articles(self, keyword: str, page: int = 1, field: Optional[str] = None) -> Dict[str, Any]:
        url = f"{self.base_url}/Articles/Search"
        params = {"keyword": keyword, "page": page}
        if field:
            params["field"] = field
        response = requests.get(url, headers=self.headers, params=params)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def get_articles_by_field(self, field: str, page_no: int = 1) -> Dict[str, Any]:
        url = f"{self.base_url}/Articles/Field/{field}"
        params = {"pageNo": page_no}
        response = requests.get(url, headers=self.headers, params=params)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def get_category_fields(self) -> Dict[str, Any]:
        url = f"{self.base_url}/Category/Field"
        response = requests.get(url, headers=self.headers)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def get_category_contents_types(self) -> Dict[str, Any]:
        """í•­ëª© ìœ í˜•(contentsType) ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
        url = f"{self.base_url}/Articles/ContentsType"
        response = requests.get(url, headers=self.headers)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def get_article_detail(self, article_id: str) -> Dict[str, Any]:
        url = f"{self.base_url}/Article/{article_id}"
        response = requests.get(url, headers=self.headers)
        # print(f"[ìš”ì²­ URL] {response.url}") # ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€
        response.raise_for_status()
        return response.json()
    
    def search_medias(self, keyword: str, page_no: int = 1) -> Dict[str, Any]:
        url = f"{self.base_url}/Medias/Search"
        params = {"keyword": keyword, "pageNo": page_no}
        response = requests.get(url, headers=self.headers, params=params)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def get_media_detail(self, media_id: str) -> Dict[str, Any]:
        url = f"{self.base_url}/Media/{media_id}"
        response = requests.get(url, headers=self.headers)
        print(f"[ìš”ì²­ URL] {response.url}")
        response.raise_for_status()
        return response.json()
    
    def _fetch_single_article(self, article_id: str) -> Optional[Dict]:
        """ë‹¨ì¼ í•­ëª© ì¡°íšŒ (ë³‘ë ¬ ì²˜ë¦¬ìš© ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            response_data = self.get_article_detail(article_id)
            article = response_data.get('article', {})
            
            # ë³¸ë¬¸ì—ì„œ HTML í…Œì´ë¸”ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
            body_data = html_table_to_structured(article.get('body', ''))
            
            # ë³¸ë¬¸ ì£¼ì„ì„ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            footnotes = clean_footnote(article.get('footNote', ''))
            
            # ê´€ë ¨í•­ëª© í•„ë“œëª… ë³€í™˜ ë° writerInfo ì œê±°
            related = convert_related_articles(article.get('relatedArticles', []))
            
            article_info = {
                'url': article.get('url'),
                'í•­ëª©ëª…': article.get('headword'),
                'ì›ì–´': article.get('origin'),
                'í•­ëª© ë¶„ì•¼': article.get('field'),
                'í•­ëª© ìœ í˜•': article.get('contentsType'),
                'ì‹œëŒ€': article.get('era'),
                'í•­ëª© ì •ì˜': article.get('definition'),
                'ìš”ì•½': article.get('summary'),
                'í‚¤ì›Œë“œ': article.get('keyword'),
                'í•­ëª© ë³¸ë¬¸': body_data.get('text', ''),
                'ë³¸ë¬¸ í‘œ': body_data.get('tables', []),
                'ì£¼ì„': footnotes,
                'ê´€ë ¨í•­ëª©': related
            }
            
            return article_info
            
        except requests.exceptions.RequestException as e:
            print(f"\nâŒ {article_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            print(f"\nâŒ {article_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def save_articles_to_json(
        self, 
        article_ids: List[str], 
        output_file: str = "articles_data.json",
        max_workers: int = 12,
        use_parallel: bool = True
    ):
        """
        ì—¬ëŸ¬ í•­ëª©ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            article_ids: ì¡°íšŒí•  í•­ëª© ID ë¦¬ìŠ¤íŠ¸
            output_file: ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 10)
            use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: True)
        """
        articles_data = []
        
        # ì¤‘ë³µ ID ì œê±°
        unique_ids = list(set(article_ids))
        total_count = len(unique_ids)
        print(f"ì´ {total_count}ê°œì˜ ê³ ìœ  IDì— ëŒ€í•´ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        if use_parallel and total_count > 1:
            # ===== ë³‘ë ¬ ì²˜ë¦¬ =====
            print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ (ìŠ¤ë ˆë“œ: {max_workers}ê°œ)")
            
            completed_count = 0
            failed_count = 0
            lock = threading.Lock()
            
            def process_with_progress(article_id: str) -> Optional[Dict]:
                nonlocal completed_count, failed_count
                result = self._fetch_single_article(article_id)
                
                with lock:
                    if result:
                        completed_count += 1
                    else:
                        failed_count += 1
                    # ì§„í–‰ë¥  í‘œì‹œ
                    print(f"\r[{completed_count + failed_count}/{total_count}] "
                          f"ì™„ë£Œ: {completed_count} | ì‹¤íŒ¨: {failed_count}", end='')
                
                return result
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  ì‘ì—… ì œì¶œ
                futures = {
                    executor.submit(process_with_progress, article_id): article_id 
                    for article_id in unique_ids
                }
                
                # ì™„ë£Œëœ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
                for future in as_completed(futures):
                    result = future.result()
                    if result:
                        articles_data.append(result)
            
            print()  # ì¤„ë°”ê¿ˆ
            
        else:
            # ===== ìˆœì°¨ ì²˜ë¦¬ =====
            print("ğŸ“ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
            
            for i, article_id in enumerate(unique_ids, 1):
                print(f"[{i}/{total_count}] í•­ëª© ì¡°íšŒ ì¤‘: {article_id}", end='\r')
                
                result = self._fetch_single_article(article_id)
                if result:
                    articles_data.append(result)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(articles_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ì´ {len(articles_data)}ê°œ í•­ëª©ì„ {output_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return articles_data
    
    def save_all_articles_from_page(self, page_no: int = 1, output_file: str = None):
        if output_file is None:
            output_file = f"articles_page_{page_no}.json"
        
        print(f"=== í˜ì´ì§€ {page_no} í•­ëª© ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì¤‘ ===")
        all_articles = self.get_all_articles(page_no=page_no)
        
        article_ids = [article['eid'] for article in all_articles.get('articles', [])]
        print(f"ì´ {len(article_ids)}ê°œ í•­ëª©ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\n")
        
        return self.save_articles_to_json(article_ids, output_file)

def _parse_id_from_url(url: str) -> Optional[str]:
    """URLì—ì„œ IDë¥¼ íŒŒì‹±í•˜ëŠ” ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜"""
    if not url:
        return None
        
    try:
        # ì˜ˆ: http://encykorea.aks.ac.kr/Contents/Item/E0000072
        extracted_id = url.split('/')[-1]
        
        # IDê°€ 'E'ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
        if extracted_id.startswith('E'):
            return extracted_id
        else:
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì²˜ë¦¬ (ì˜ˆ: E0000072?view=...)
            match = re.search(r'(E\d+)', extracted_id)
            if match:
                return match.group(1)
    except Exception:
        pass
    return None

def extract_ids_from_excel(file_path: str) -> List[str]:
    """
    Excel(.xlsx) íŒŒì¼ì„ ì½ì–´ Cì—´(3ë²ˆì§¸ ì—´)ì˜ URLì—ì„œ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if openpyxl is None:
        print("âŒ 'openpyxl' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜ ëª…ë ¹ì–´: pip install openpyxl")
        return []

    ids = []
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []

    try:
        print(f"ğŸ“— ì—‘ì…€ íŒŒì¼ ì—´ê¸°: {file_path}")
        # data_only=True: ìˆ˜ì‹ì´ ì•„ë‹Œ ê°’ë§Œ ì½ìŒ
        wb = openpyxl.load_workbook(file_path, data_only=True, read_only=True)
        sheet = wb.active # ì²« ë²ˆì§¸ ì‹œíŠ¸ í™œì„±í™”
        
        # iter_rowsë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì½ê¸°
        # min_row=2 (í—¤ë” ê±´ë„ˆë›°ê¸°), min_col=3, max_col=3 (Cì—´ë§Œ)
        for row in sheet.iter_rows(min_row=2, min_col=3, max_col=3, values_only=True):
            if row and row[0]:
                url = str(row[0]).strip()
                article_id = _parse_id_from_url(url)
                if article_id:
                    ids.append(article_id)
        
        wb.close()
        print(f"âœ… ì—‘ì…€ íŒŒì¼ ì½ê¸° ì„±ê³µ")
        
    except Exception as e:
        print(f"âŒ ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    return ids

def extract_ids_from_csv(file_path: str, encoding: str = None) -> List[str]:
    """
    CSV íŒŒì¼ì„ ì½ì–´ Cì—´(3ë²ˆì§¸ ì—´)ì˜ URLì—ì„œ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    ids = []
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []

    encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']
    if encoding:
        encodings_to_try = [encoding]

    for enc in encodings_to_try:
        try:
            temp_ids = []
            with open(file_path, 'r', encoding=enc) as f:
                reader = csv.reader(f)
                headers = next(reader, None) # í—¤ë” ìŠ¤í‚µ
                
                for row in reader:
                    if len(row) > 2:
                        url = row[2].strip()
                        article_id = _parse_id_from_url(url)
                        if article_id:
                            temp_ids.append(article_id)
            
            print(f"âœ… ì¸ì½”ë”© ê°ì§€ ì„±ê³µ: '{enc}'")
            return temp_ids

        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"âŒ '{enc}' ì‹œë„ ì¤‘ ì½ê¸° ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
            
    print(f"âŒ ëª¨ë“  ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return []

def extract_ids_from_file(file_path: str) -> List[str]:
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ì¶”ì¶œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    ext = os.path.splitext(file_path)[1].lower()
    
    if ext == '.xlsx':
        return extract_ids_from_excel(file_path)
    elif ext == '.csv':
        return extract_ids_from_csv(file_path)
    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}")
        return []

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # API í‚¤ ì„¤ì •
    API_KEY = "A5931F50-59E2-4679-93CF-5858EC174900"
    
    # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = HistoryAPIClient(API_KEY)
    
    # ==========================================
    # íŒŒì¼ ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)
    # .csv ë˜ëŠ” .xlsx íŒŒì¼ ê²½ë¡œ ì…ë ¥
    # ==========================================
    # ì˜ˆ: "data.xlsx" ë˜ëŠ” "data.csv"
    input_file_path = "í•œêµ­í•™ì¤‘ì•™ì—°êµ¬ì›_í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „_20240130.xlsx" 
    output_json_path = "crawling/results.json"
    
    print(f"=== íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file_path} ===")
    
    # ì¹´í…Œê³ ë¦¬ í•„ë“œ ì¡°íšŒ
    fields = client.get_category_fields()
    print("\n=== ì¹´í…Œê³ ë¦¬ í•„ë“œ ëª©ë¡ ===")
    print(json.dumps(fields, ensure_ascii=False, indent=2))
    
    # í•­ëª© ìœ í˜•(contentsType) ì¡°íšŒ
    contents_types = client.get_category_contents_types()
    print("\n=== í•­ëª© ìœ í˜•(ContentsType) ëª©ë¡ ===")
    print(json.dumps(contents_types, ensure_ascii=False, indent=2))
    
    # # 1. íŒŒì¼ì—ì„œ ID ì¶”ì¶œ (ìë™ ê°ì§€)
    # target_ids = extract_ids_from_file(input_file_path)
    
    # if target_ids:
    #     print(f"\nğŸ” ì¶”ì¶œëœ ID ê°œìˆ˜: {len(target_ids)}ê°œ")
    #     print(f"   (ì²« 5ê°œ ì˜ˆì‹œ: {target_ids[:5]})")
        
    #     # 2. ì¶”ì¶œëœ IDë¡œ API ì¡°íšŒ ë° ì €ì¥
    #     print("\n=== API ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    #     client.save_articles_to_json(target_ids, output_json_path)
    # else:
    #     print("\nâš ï¸ ì²˜ë¦¬í•  IDê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œì™€ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")